[{"title":"Generating Stack Machine Code in LLVM","url":"lialan.github.io/2020/02/06/stack-machine-LLVM/","content":"<p>I am working on a project to bring LLVM to the blockchain world. More specifically, I am building an EVM code generation framework on LLVM. There are some similar works on going. Such as WebAssembly backend on LLVM. </p>\n<p>I am trying to keep an record of the technical decisions I made when doing the implementation. You can have a look at it at <a href=\"https://github.com/etclabscore/evm_llvm/wiki/files/Generating_stack_machine_code_using_LLVM.pdf\" target=\"_blank\" rel=\"noopener\">here</a></p>\n<p>You can also take a look at the code, it is <a href=\"https://github.com/etclabscore/evm_llvm\" target=\"_blank\" rel=\"noopener\">open source</a>. </p>\n","categories":[],"tags":["Compiler"]},{"title":"Impressions from LLVM Dev Meetup 2019","url":"lialan.github.io/2019/10/29/Impressions-from-LLVM-Dev-Meetup-2019/","content":"<h1 id=\"Overall-impressions\"><a href=\"#Overall-impressions\" class=\"headerlink\" title=\"Overall impressions\"></a>Overall impressions</h1><p>This year, there were 630 LLVM developers from all around the world joining together to discuss compiler technologies that nobody else really cares. As someone who works on compilers, you cannot miss this opportunity to join this annual circlejerk. So I paid for the venue ticket, booked a 2-night AirBnb, and took the red eyes to San Jose.</p>\n<p>So what did I find? Compiler engineers are still in high demand in 2019. Job hire flyers from big players such as Google, Facebook, Apple, Microsoft… you name it, are all over the place. Good luck finding a good compiler developer in the Bay area if your company thinks paying USD$300k-500k for a senior engineer is too much. Very unfriendly for infrastructure startups. I miss the time when Borland was still alive and kicking, and people could still start a business monetizing on archaic compiler technologies.</p>\n<p><img src=\"/media/llvm_meetup.png\" alt></p>\n<h1 id=\"MLIR\"><a href=\"#MLIR\" class=\"headerlink\" title=\"MLIR\"></a>MLIR</h1><p>MLIR is obviously the single hottest topic in the meetup. Tuesday’s MLIR roundtable took up most space of the room. Developers are eager to know more from the TensorFlow team.</p>\n<p>(If you want to know more about MLIR, here is the a background literature: <a href=\"https://ai.google/research/pubs/pub48035\" target=\"_blank\" rel=\"noopener\">https://ai.google/research/pubs/pub48035</a> )</p>\n<p>MLIR’s adaptability and versatility is immensely useful to compiler developers. Although the most obvious and immediate use case of MLIR is definitely machine learning compiler optimizations, and frontend development such as building DSLs, it can also be adopted to both the frontend codegen and the machine level codegen. For example, no one forbids you to convert machine IR to MLIR and do some specific analysis and optimizations then convert it back. It will not take too long to see LLVM backends utilize MLIR to do more sophisticated optimizations.</p>\n<p>Chris Lattner specifically mentioned that there are no immediate intends to replace LLVM IR with MLIR, at least for the coming year or two. This is true, people are still investing heavily into LLVM IR infrastructure, and MLIR still needs some time to mature itself. However, I feel that given the strong similarity (MLIR being a superset of LLVM IR actually), soon there will likely be attempts to migrate works to MLIR — if people think MLIR is a better vehicle for that particular job.</p>\n<p>Developers tries to do specific optimizations in frontends such as Clang because when lowering to LLVM IR, some information on the AST is lost and unretrievable. Metadata is a good way to store additional information but the frontend cannot enforce backends on taking actions on metadata. That is the reason there are dedicated IRs for specific languages: Julia has SIR, and Rust has HIR and MIR, et cetera. Using MLIR could make creating new programming languages easier without the need for a dedicated higher-level IR — parse whatever you have in the source code into AST, lower it to MLIR, do a bunch of analysis and optimizations, lower to LLVM IR, sent it to a backend, done. Never been simpler.</p>\n<p>So can I use MLIR today? In production mode? Eric Schweitz from NVIDIA presented their work on using MLIR to optimize the good old, steam-driven Fortran. They specialized MLIR to their FIR dialect and do a bunch of memory and loop optimizations. So, yeah, MLIR is usable now, if you don’t mind developer team’s rapid version iterations break your build.</p>\n<h1 id=\"GlobalIsel\"><a href=\"#GlobalIsel\" class=\"headerlink\" title=\"GlobalIsel\"></a>GlobalIsel</h1><p>The opening keynote were about the progress of GlobalIsel (Global Instruction Selection). This work for now is mostly about reducing compile time. Engineers from Apple showcased the approach on stage, and announced that the iOS 13 is compiled using GlobalISel enabled, without any other bluffing PR bullshit. It was epic.</p>\n<p>Currently, most backends are using traditional SelectionDAG module for instruction selection, which requires the SelectionDAG IR. The GlobalISel wants to remove the SelectionDAG IR component once and for all and go straightly to Machine IR, meanwhile doing instruction selection on a bigger scale than basicblocks. Some background info can be found here: <a href=\"https://llvm.org/docs/GlobalISel.html\" target=\"_blank\" rel=\"noopener\">https://llvm.org/docs/GlobalISel.html</a></p>\n<p>Like SelectionDAG, GlobalISel will have a declarative language based on tablegen. This is why I think compiler technology is great software techniques: After years of research and development, abstractions after abstractions, target-specific components are more and more declarative.</p>\n<p>The adoption of GlobalIsel will be gradual. A backend could start with full SelectionDAG approach, and incrementally implement their strategy in GlobalISel. The GlobalISel module will be the first option, and in case of failure, falls back to SelectionDAG, without breaking the build. The backend can continue to improve the GlobalISel until it does not fallback anymore before it can totally ditch SelectionDAG.</p>\n<p>Apple reported a huge compile time improvement after switching. New backend developments should start considering adopting GlobalISel.</p>\n<h1 id=\"Code-Generation\"><a href=\"#Code-Generation\" class=\"headerlink\" title=\"Code Generation\"></a>Code Generation</h1><p>MediaTek presented their work on using Souper (<a href=\"https://arxiv.org/abs/1711.04422\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/abs/1711.04422</a>) to do target machine specific peephole superoptimization. Because Souper operates on LLVM IR, there is a missing link between the LLVM IR and actual machine code generated. The team came up with a cost prediction model utilizing LLVM MC Analyzer to map between particular LLVM IR structures with generated machine code, hence finds a way to back track the target machine specific cost model to LLVM IR. This idea could be implemented in Machine IR level, yielding a more precise and more target-specific cost model mapping between MIR and generated machine code. Furthermore, it could be designed to be a generic peephole pass with some target-specific elements, and those target-specific elements could be implemented using tablegen.</p>\n<p>Jin Lin from Uber presented their approach to reduce the Uber iOS app code size at LTO time. The idea is quite simple — wait until link time when you have the full scope visibility of the program, then do as much outlining and function eliminating/combining as possible. This worked pretty well on Uber app. So what is the takeaway from this? A compiler engineer is invaluable even for a company like Uber. The improvements on toolchain is greatly impactful on the end product’s performance.</p>\n<p>Another very practical session I attended was presented by Alexandre Ganea from Ubisoft on reducing build time on Windows machines. When building a game from scratch takes longer than anyone can bear, they fine tuned the incremental build system to make an incremental build less than 3 minutes. It is another example showcasing how a toolchain developer can drastically improve the productivity of the whole team without change of routines. Simply impressive.</p>\n<p>I met with Thomas the WebAssembly LLVM backend maintainer, and Dimitry who works on the TON (The Telegram blockchain) compiler, at Dimitry’s session where he wants everybody’s input on how we can produce better stack machine codes. We all agree that the LLVM could use some generic infrastructure to improve the code quality of stack machine code generation. We are thinking to start our collaborated contribution with a RFC.</p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>This is a conference for sharing practical, no-bullshit engineering ideas. I like it very much — the talks are so practical I can immediately relate those topics to my work scenarios. I paid for two conferences this month: Devcon5 and LLVM Dev Meetup. Although Osaka is a great city to visit, I still think my experiences at Devcon5 was very underwhelming. On the other hand, the LLVM meetup, being an only 2-day conference, has much less bullshit and much more good ideas that I can immediately relate to. I will definitely come back next year.</p>\n<blockquote>\n<p>Note: this was originally published at: <a href=\"https://medium.com/@lonelydove/impressions-from-llvm-dev-meetup-2019-954c3887a675\" target=\"_blank\" rel=\"noopener\">https://medium.com/@lonelydove/impressions-from-llvm-dev-meetup-2019-954c3887a675</a></p>\n</blockquote>\n","categories":[],"tags":["Compiler","English"]},{"title":"A talk on the EVM LLVM project","url":"lialan.github.io/2019/10/05/EVM-LLVM-deck-from-ETCSummit/","content":"<p>I gave a talk about one of my project, the EVM LLVM backend on Oct 3rd, 2019 in Vancouver. The talk was about the impacts of a compiler infrastructure to the blockchain community and the engineering details in the implementation.</p>\n<p>Here is the link to the <a href=\"http://htmlpreview.github.io/?https://github.com/lialan/reveal.js.slides/blob/master/index.html\" target=\"_blank\" rel=\"noopener\">slides</a></p>\n<p>Here is the <a href=\"https://www.youtube.com/watch?v=TNsWR7jXllQ:3:32\" target=\"_blank\" rel=\"noopener\">youtube link</a></p>\n<p><img src=\"/media/ETCSummit2019.jpeg\" alt=\"Drinking maple syrup\"></p>\n","categories":[],"tags":["Compiler","Blockchain","English"]},{"title":"A Zilliqa Technical Report","url":"lialan.github.io/2018/12/01/zillqa-report/","content":"<p>Note: This article is firstly published on FBG X’s WeChat public account. </p>\n<hr>\n<p><img src=\"/media/15393575754598.jpg\" alt></p>\n<p>One of the primary selling points of the Zilliqa team is that at the launch of the main net, sharding will be supported. In fact, sharding is best served launching with the network, or the transition to the sharded state imposes a difficult hardfork.</p>\n<h2 id=\"Designs-of-Sharding-Mechanism\"><a href=\"#Designs-of-Sharding-Mechanism\" class=\"headerlink\" title=\"Designs of Sharding Mechanism\"></a>Designs of Sharding Mechanism</h2><p>In a distributed ledger system, there are 4 components that can be sharded:<br><img src=\"/media/15393575607792.jpg\" alt></p>\n<p>Researchers and developers from Ethereum want to implement state sharding once and for all, and that imposes many difficulties in both design and implementation. Since the team is still iterating designs, it is expected that we will not see its sharding mechanism comes to live until 2020 or later.</p>\n<h3 id=\"Sharding-Mechanism-in-Zilliqa\"><a href=\"#Sharding-Mechanism-in-Zilliqa\" class=\"headerlink\" title=\"Sharding Mechanism in Zilliqa\"></a>Sharding Mechanism in Zilliqa</h3><p>The design of Zilliqa’s sharding mechanism is such that its phase one’s goal is to only shard transaction (including payment and smart-contract transaction) execution, but synchronize the machine state across shards. From the perspective of engineering, maintaining the same state across shards dramatically reduces the design complexity and indeed improves execution parallelism and speed compared to a non-sharding mechanism.</p>\n<p>Zilliqa has been questioned about its sharding, mostly about its non-state sharding approach. We agree that a non-state sharding is not a complete and comprehensive solution to scalability. A non-state solution involves extensive cross-shard interaction, including synchronization, propagation of state updates, etc. The to-and-from communication between shards increases latency and decreases responsiveness, hence impeding further performance gains.</p>\n<p>In the state-sharding approach, since different shards store the states exclusively, once a smart contract transaction involves in two states in different shards, cross-shard communication will be triggered. Frequent cross-shard communication will cause significant overhead and delay for transaction execution. The second challenge with state sharding is data availability. Once a shard is offline due to attacks, the states stored in that shard is not accessible by the other shards and transactions related to that shard won’t be executed anymore. The last hurdle is reshuffling for state sharding. To ensure the security guarantee, the random assignment of nodes to shards should be performed periodically. As each shard maintains the separate set of states, every node new to a shard has to sync with the old nodes. The synchronization of states will introduce a non-negligible overhead to the entire system.</p>\n<p>Ideally and economically, nodes in a shard should store only information relevant to itself and only when necessary, and irrelevant data are fetched from other shards. This approach minimizes the storage and reduces disk space requirement by a factor of the number of shards – at least that is the optimistic picture. However, we argue that such a design scheme splits the network and reduces mining power attack resistance proportionally. The more shards a network is split into, the easier an attacker will succeed. Take Ethereum for example, as of September 28th there are about 15,000 nodes in total running worldwide. Such number wouldn’t stand a small range attack should the network is split into 100 shards. </p>\n<p>Yet the most attacked point of such non-state sharding mechanism is that: replicating the complete network state across shards increases the storage burden of each node in the network. Without an efficient storage fee schedule, the node’s storage space will explode exponentially and go out of control. </p>\n<p>Zilliqa’s response to the storage explosion problem is not to store transaction logs on each of the nodes – only the latest ledger state is synchronized. This approach eliminates the disk space growth required to store ledger history and is only proportional to the number of total users (usually a very controllable number). In the meantime, Zilliqa has formed the partnership with distributed storage projects to seek solutions by offloading storage to the distributed storage network. Users can choose their preferred decentralized storage system and pay for the storage used by the smart contract running atop Zilliqa. Due to the three challenges mentioned above, the state-sharding approach can mitigate the storage problem, but won’t be the ultimate solution to it. A standalone decentralized storage system is necessary to all the scalable blockchain platforms.</p>\n<p>Randomness is the key to achieve a solution in this exact case. Zilliqa’s solution is to shuffle nodes across shards periodically. Simply put, by doing this, a malicious player is unable to concentrate resources to attack a single thread of the network – its computing power is forced to be dispersed randomly into different shards. Meanwhile, every shard has at least 600 nodes, which ensures the relative decentralization within a shard and guarantees the high-security level that with high probability (over 99.999999%) the shard won’t fail.</p>\n<p>One of the most difficult problems for sharding is to handle smart contracts, as the same smart contract may be executed in different shards simultaneously to cause data inconsistency issue, e.g., airplane and hotel problem. For state-sharding solutions, we can leverage 2-phase commit protocol or lock mechanism to ensure the sequential execution of different transactions of the same smart contract across shards. For Zilliqa, they employ an intelligent transaction assignment mechanism. They categorize transactions into payment transactions and simple contract transactions and complex contract transactions including recursively calling contracts. For payment transactions, they will be assigned to shards based on their senders’ addresses. For simple contract transactions, the assignment is relying on the sender and contract’s addresses. For the complex contract transactions,  a dedicated shard will handle them. Since the majority of the transactions are belonging to the first two categories, all the transactions including smart contract can be properly executed in parallel in different shards. As the future work, Zilliqa’s research team is working on optimizing the assignment of complex contract transactions with language support.</p>\n<p>We’ve interviewed Zilliqa’s CTO, Yaoqi Jia, about its engineering plan. He thinks Zilliqa’s approach is engineering appropriate, in that the delivery timeline strikes a balance between quality and delivery dates. </p>\n<p>He also mentioned that people tend to overlook the difficulty of the design and implementation of an ideal sharding mechanism inside a blockchain system. We agree with Yaoqi’s opinion, sharding is not an easy mission.</p>\n<h3 id=\"Alternative-Designs\"><a href=\"#Alternative-Designs\" class=\"headerlink\" title=\"Alternative Designs\"></a>Alternative Designs</h3><p>Zilliqa’s team has over 20 designs of sharding, the one plan they picked to implement is relatively practical (yet still quite complicated, as there is no simple solution to sharding) because they want to reduce the complexity when implementing and maintaining. </p>\n<p>Such a design enables the comprehensive sharding mechanism for payment and smart contract transactions, but it requires good network throughput to handle a large number of transactions, which is the same issue for all the scalable blockchains. </p>\n<h2 id=\"New-Smart-Contract-Platform\"><a href=\"#New-Smart-Contract-Platform\" class=\"headerlink\" title=\"New Smart Contract Platform\"></a>New Smart Contract Platform</h2><p>Instead of reusing existing smart contract execution platform such as Ethereum virtual machine, the Zilliqa blockchain is proposing its own smart contract solution. The smart contract platform is based on a newly proposed intermediate-level language called Scilla.</p>\n<p>We have observed that Scilla diverts from Zilliqa’s original proposal of following a dataflow programming paradigm. The data flow paradigm was proposed in the 1960s by Jack Dennis and is capable of handling large parallelism of data processing. But such streaming paradigm is best implemented in large scale, macro systems. Smart contract executions, on the other hand, is at a much smaller scale and is highly localized. We think that Zilliqa’s adaptation of functional paradigm instead of data flow paradigm is the right choice, as functional programming paradigm is much more widely used and verified.</p>\n<p>The new Scilla language is essentially a variation of an OCaml sub-language with an intrinsic support of functional programming paradigm.Unlike traditional imperative paradigm languages, functional languages usually provide additional safeguards to help writing secure computer programs. As a post has already pointed out:</p>\n<p>Scilla imposes a structure on smart contracts that will make applications less vulnerable to attacks by eliminating certain known vulnerabilities directly at the language-level. Furthermore, the principled structure of Scilla will make applications inherently safer.</p>\n<p>We have the following observations:</p>\n<ul>\n<li>Written in OCaml itself, the Scilla execution engine is natively compatible with Scilla language itself, making the interaction between the engine and the application more accessible.</li>\n<li>Compared with Ethereum VM bytecode, the Scilla language is a higher-level language, meaning its operators express more abstract semantics. In fact, you can look into their GitHub repository and read their smart contract demos, and can start writing your own contracts. Also, compared to other platforms, debugging in Scilla is more viable and easier, making a boost in productivity.</li>\n<li>The Succinctness of the semantics and the functional paradigm makes coding smart contracts in Scilla platform safer and securer. The Scilla language is designed to be an intermediate-level language, usually, an intermediate language in a compiler and virtual machine system is not intended to be operated by a human, but Scilla’s readability makes it operable by a human. In fact, we think developers are able to write smart contract code in Scilla after a short adjusting period.  </li>\n<li>Safety and security mechanism such as formal verification can be easily implemented for Scilla. It will be extremely easy to have Scilla platform along with its decentralized applications proven using formal verification. From this aspect, Scilla will later become a safe and secure executing platform at release and hence an excellent addition to the Zilliqa platform.</li>\n</ul>\n<p>We believe that the time-tested functional programming paradigm will be widely used in smart contract world because it needs a safe, principled and structural, highly productive language to be the fundamental component of the smart contract environment. Of course, there might be several difficulties in implementing the platform:</p>\n<ul>\n<li>Due to its high-level abstraction, some of Scilla’s operations do not have a fixed actual cost. Take the traditional functional function <code>fold</code> as an example, the cost of the actual machine execution varies from 0 to infinity. This makes implementing an accurate gas fee system difficult. In such case, traditional per operation fee schedule might not be sufficient to cover all the cases. Also using empirical solution – using a predefined fixed price might not fit for all real-world cases, and might cause vulnerabilities in economical attacks. In such case, following a smart contract executing fee schedule first introduced in Ethereum might not be sufficient, the team needs to look for a new fee-based system for their system to make it fair enough to use and robust enough to hold against potential economical attacks. </li>\n<li>The proposed smart contract solution, including the Scilla intermediate language and its virtual machine and a bundle of toolkits, requires a large amount of work before it can be published. Not only does the team need to rebuild the execution engine from scratch (the team might not be able to find or reuse open source components), but also the community and the team have to build developer tools such as Scilla compilers, transpilers (a compiler that takes other smart contract source code such as Solidity contracts and outputs Scilla code), etc. A large amount of workload demands a tighter relationship between the developer team and the community. </li>\n<li>The implementing of Scilla is a starting point of functional languages entering smart contract infrastructure. The paradigm shift of the language requires a shift of the developer’s skills as well. Without supporting development tools such as transpilers, developers not coming from a functional programming background or unfamiliar with it will face a period of adjustment before they can write correct and efficient Scilla contract code.  </li>\n<li>The team plans to launch their main net in December/Jan (delayed from September this year) and at the time of launch, the intermediate-level language based smart contract execution engine and some dev tools like IDE will be ready. All other decentralized application (Dapp) development support, including Solidity, javascript programming capabilities, debug support, etc, will be arriving later than Zilliqa’s main net launch.</li>\n<li>To boost their early ecosystem building, the team has set up an ecosystem grant of 5 million US dollars in total, for incentivizing community members to help alleviate the gap by starting to write Scilla based smart contracts. Overall, Zilliqa’s main net launch is likely to have a few Dapps running. Our observation is that the framework can support essential applications, such as launching a token or ICO. Zilliqa will provide formal verification of those applications to ensure correct execution.</li>\n</ul>\n<p>Something worth mentioning: some developers have expressed concerns over their non-Turing complete language design might impact the development of Dapp on Zilliqa. As far as we know, the designers of Scilla removes looping operations in the language so that the program won’t dead loop or consume too many resources. Removing looping operations will result in a straightforward execution flow, making the compiler and virtual machine doing optimizations easier, but might cause the code size to increase (which is not a good thing if the contract is stored on-chain). </p>\n<p>You can try and experiment Scilla language at <a href=\"https://scilla-lang.org/\" target=\"_blank\" rel=\"noopener\">https://scilla-lang.org/</a>. At this moment, it comes with an online IDE at <a href=\"https://savant-ide.zilliqa.com/\" target=\"_blank\" rel=\"noopener\">https://savant-ide.zilliqa.com/</a>. The IDE helps you check your program parsing errors and type checking. Currently, there are several example smart contracts that you can do your experiments on.</p>\n<p>The hands-on experience of Scilla is overall fascinating. As a person from a functional programming background, I like Scilla’s language features such as pattern matching and algebraic data types (ADTs). They are natural to use and as convenient as coding in other functional languages. Using Scilla, one does not feel like writing in an intermediate-level language such as LLVM IR. Communications are done via messages, which are simply defined in JSON format. Language primitives such as <code>send</code> are exposed to users. A standard utility function library is provided.</p>\n<h2 id=\"Team-and-community\"><a href=\"#Team-and-community\" class=\"headerlink\" title=\"Team and community\"></a>Team and community</h2><p>The team has about 20 people, the majority of the group are developers. 4 People are working on their new smart contract language support, and the rest of the dev team are working on the core protocol development and prepare for the main net launch. </p>\n<p>The team is development-oriented and result-driven. The tech team has enthusiastic and capable. On the other hand, their business development and marketing team (which is only a small part of the whole team) is starting to ramp up their effort so as to deliver more results around the adoption of the platform. Good news is that they just hired CMO from Google and Vice President for BD to improve their marketing and business strategy. Meanwhile, they also started a project called Proton with Mindshare, Mediamath, Rubicon Project, Integral Ad Science (IAS) and Underscore CLT, which all are big players in the digital advertising market. </p>\n<p>Zilliqa online development community is active, and members are enthusiastic, as described by Yaoqi Jia, Zilliqa’s Head of Technology. The team has established R&amp;D grants of 5 million US dollars in total for the community Now community members can get the grant or reward by building tools like wallets, Dapps and other components like state channel for Zilliqa.</p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li>The Zilliqa Technical Whitepaper;</li>\n<li>“Limitations of Zilliqa’s Sharding Approach” Alexander Skidanov;</li>\n<li>“The Many Faces of Sharding for Blockchain Scalability” Bitcoin Magazine;</li>\n<li>“Provisioning Sharding for Smart Contracts: A Design for Zilliqa” Amrit Kumar;</li>\n<li>“Scilla: A Smart Contract Intermediate-Level Language” Ilya Sergey, Amrit Kumar, and Aquinas Hobor;</li>\n<li>“Provisioning Sharding for Smart Contracts: A Design for Zilliqa” Amrit Kumar;</li>\n<li>“Diving into Scilla: A Safe-by-Design Smart Contract Language” Adrian Irimia;</li>\n</ul>\n<h2 id=\"Acknowledgments\"><a href=\"#Acknowledgments\" class=\"headerlink\" title=\"Acknowledgments\"></a>Acknowledgments</h2><p>We would like to thank:</p>\n<ul>\n<li>Dr. Yaoqi Jia, Zilliqa’s CTO,</li>\n<li>Dr. Xinshu Dong, Zilliqa’s CEO, </li>\n<li>Dr. Amrit Kumar, Zilliqa’s Head of Research<br>for the help of clarifying the technology details.</li>\n</ul>\n","categories":[],"tags":["Blockchain","English"]},{"title":"Telegram Virtual Machine -- A Technical Review","url":"lialan.github.io/2018/11/13/Telegram-VM/","content":"<p>Author: It is an unfinished technical review of the telegram virtual machine specification. It is not intended to be interpreted as market analysis in anyway.</p>\n<h2 id=\"The-Stack-Machine\"><a href=\"#The-Stack-Machine\" class=\"headerlink\" title=\"The Stack Machine\"></a>The Stack Machine</h2><!-- comparison -->\n<p>Here is a very simple ISA comparison chart among Telegram VM, Ethereum VM and a RISC-V architecture:</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Telegram VM</th>\n<th>Ethereum VM</th>\n<th>RISC-V</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Machine Type</td>\n<td>stack machine</td>\n<td>stack machine</td>\n<td>register machine</td>\n</tr>\n<tr>\n<td>Extendable</td>\n<td>yes</td>\n<td>no</td>\n<td>yes</td>\n</tr>\n<tr>\n<td>Encoding Type</td>\n<td>bitcode</td>\n<td>bytecode</td>\n<td>machine code</td>\n</tr>\n<tr>\n<td>Exception Handling</td>\n<td>exception handler</td>\n<td>unsupported</td>\n<td>exception handler</td>\n</tr>\n<tr>\n<td>Intended Implementation</td>\n<td>Software</td>\n<td>Software</td>\n<td>Hardware</td>\n</tr>\n<tr>\n<td>Instruction Length</td>\n<td>variable length</td>\n<td>fixed length</td>\n<td>variable length</td>\n</tr>\n</tbody>\n</table>\n<p>The Telegram virtual machine is, as the team’s document said, not intended to be implemented as a hardware platform. This gives them some freedom while designing the details of the machine.</p>\n<!-- ISA -->\n<p>When designing the ISA (Instruction Set Architecture) of a machine/virtual machine, one considers the trade off between the performance and efficiency. The main reason that the team has chosen the stack machine model over a register machine model, as the stack machine model has superior code density and extensibility. This is because the cost of storage on the blockchain is millions of times higher than a cloud storage. (In fact, even though it is already extremely expensive, it is still too cheap.) In an effort to minimize the cost of on-chain storage, a design of dense opcodes is the key to reduce cost. But the team did not go too far as to fully minimize the code size, as a size-optimal design will inevitably and greatly increase the complexity of the design and implementation of the virtual machine. The team is trying to strike a balance between achieving good code size and practical virtual machine implementation. </p>\n<!-- number of opcodes -->\n<p>The Ethereum Virtual Machine is a bytecode stack machine, meaning all of its opcodes are one byte in size. With the code space of $2^8$, the instruction set is small and very simple, making the Ethereum VM simple, lightweight. This simplistic design greatly reduced the engineering effort required to make Ethereum smart contracts executable. The Telegram VM, on the other hand, is noticeably more sophisticated in this aspect. TVM has way more opcodes as its opcode length varies from one to three bytes. We expect the effort of development of the TVM will take quite a while, and the work to maintain TVM will be non-trivial as well. </p>\n<!-- extensibility -->\n<p>TVM has a specific status register called <code>codepage</code> register in each of the continuation. the <code>codepage</code> register determines the revision of the opcode that will be used in that specific environment. This creates nearly unlimited extensibility of the instruction set. Introducing a new revision of TVM opcodes will not have to deprecate existing smart contracts deployed on the blockchain, it can simply turn on another codepage so that smart contracts compiled using different revisions of the opcode can coexist on the blockchain, without having compatibility issues. </p>\n<!-- encoding -->\n<p>In order to save the precious on-chain storage space, primitive data types such as integers are </p>\n<h2 id=\"Discussions-on-Programming-Model\"><a href=\"#Discussions-on-Programming-Model\" class=\"headerlink\" title=\"Discussions on Programming Model\"></a>Discussions on Programming Model</h2><!-- determinism -->\n<p>Smart contract execution environment, unlike other execution environment, needs execution determinism – the capability to get the exact same result on different runs on different machines, when given a same starting environment and input arguments. Exact reproducibility is indispensable for blockchain-based smart contracts, otherwise a consensus will be hard to achieve. Usually, the execution of a computer program is deterministic (the big topic of random number generation is actually pseudo random number generation and asks for external entropy input), but if we consider a more complicated execution environment, such as multi-threading, limited memory space, different execution architecture, arithmetic precision, existence of an oracle, etc, execution determinism is difficult to implement.</p>\n<!-- Reference counting -->\n<p>In order to implement determinism, reference counting, instead of a garbage collector, is used in TVM to manage memory. Reference counting as a memory management mechanism uses a reference counter field to track the reachability of each of the created heap objects. When an object’s reference count is not zero, it means that it is reachable by other objects, so its memory is not to be reclaimed. The obvious reason is that reference counting in a single thread environment can achieve determinism, and is the easiest way to implement. A garbage collector, on the other hand, although can be much more efficient, is indeterministic in execution (the <code>deterministic</code> word in the garbage collector related field refers to a different context – determinism of pause time upper bound) and is significantly more complicated to implement. For example, it takes extreme engineering efforts to make a garbage collector of a sophisticated VM such as java VM to achieve maturity. Existing time-proven GC, such as G1 of OpenJDK, or IBM’s J9 GC, all took several major version iterations before it became stable. </p>\n<!-- No circular references, and the \"value\" paradigm -->\n<p>Inside the Telegram Virtual Machine, there is an “everything is a value” paradigm implemented for their memory object model. Usually in some other programming environment such as Java or Python, when one wants to use an object in another context, one can use a new “pointer” or “reference” to “point to” the object as a handle to be passed to other execution logics. Internally in TVM, making 2 pointers point to a same object is impossible –instead a copy of the object is created (don’t worry about efficiency, there is an optimization called “copy-on-write”, which does lazy copy only when it is needed). In another sense, each object has a sole “owner”. </p>\n<!-- Consequences -->\n<p>The consequence of adapting this paradigm, is that *there will be no circular reference**. A circular reference is a situation where an object <code>A</code> refers to an ancestor of itself. In such cases reference counting would fail to operate due to the fact that object <code>A</code>‘s reference counter will never reach zero, even when it is not being used anymore. Circular reference is a major limitation of the reference counting in memory management. Usually, when a language adapts reference counting, it is up to the developer to avoid creating circular references in their program because it is impossible for static analysis to infer all circular referencing structures in the program. When circular references are completely eliminated, reference counting can free up the occupied space immediately and accurately – when the program is serialized and stored on-chain, this feature saves space.</p>\n<!-- TODO: draw a graph? -->\n<p>The side effect of all those choices and designs, is that we now have a slightly changed programming model. In order to optimize smart contract performance, developers will have to adapt to it, before optimization can remove the additional copies.</p>\n<!-- continuations -->\n<h2 id=\"Discussions-on-Storage-Management\"><a href=\"#Discussions-on-Storage-Management\" class=\"headerlink\" title=\"Discussions on Storage Management\"></a>Discussions on Storage Management</h2><!-- storage paradigm -->\n<p>When data is to be recorded on a persistent storage such as on-chain storage, it needs to be serializable and de-serializable. In Telegram Virtual Machine, It adopts an “everything is a bag of cells” paradigm and deploys a tree-structural scheme (with copy-on-write optimization, the actual representation is a <code>directed acyclic graph (DAG)</code>) to manage storage space. Nodes on in the trees are called <code>cells</code>, and three basic operations: <code>create</code>, <code>serialize</code> and <code>deserialize</code> are defined. </p>\n<!-- serializability -->\n<p>Because TVM needs to execute smart contracts on the TON Blockchain, we have to make sure the code and the data are both serializable and de-serializable (hence “everything is a bag of cells”). In TVM, data are represented in the form of algebraic data types (ADT) which the team adopts from functional programming paradigm. ADT has a very neat feature: an arbitrary value of ADT can be successfully serialized and put on persistent storage, a convenient choice of a blockchain. Also, the TVM code itself is also a complex ADT which in turn, can be represented by a tree of cells as well. As such, TVM has achieved both the code and data serializability.</p>\n<!-- comparison with EVM's serializability -->\n<p>In Ethereum VM, similar to our binary computation, smart contract code is compiled into a string of byte codes and a global symbol table. instead of a tree of cells. In TVM, </p>\n<!-- indexing cells -->\n<p>Each cell has a sequence of hashes associated to it.</p>\n<p><code>representation hash</code> associated, which is the SHA256 of its content in serialized format.</p>\n<h2 id=\"Discussions-on-language-semantics\"><a href=\"#Discussions-on-language-semantics\" class=\"headerlink\" title=\"Discussions on language semantics\"></a>Discussions on language semantics</h2><!-- integers arithmetics -->\n<p>TVM supports native mathematical representation of integers. Although integers are represented in 257-bit format internally, but TVM does not allow implicit/automatic conversions. Arithmetic overflow/underflow are also illegal in TVM – in case overflow/underflow occurs, an arithmetic exception is raised, and an exception handler will take over the control flow and take action. This is a much welcomed semantic feature in smart contract world, where developers has accidentally created a large number of integer overflow cases in their contracts in platforms missing a guard for accidental overflows. </p>\n<!-- Everything as a value -->\n<p>In order to achieve “everything as a value” in the VM, TVM slightly changed the way object is being handling. As is said in the reference: When one tries to create a circular reference between two cells, say <code>A</code> and <code>B</code>, we will obtain a new cell <code>A&#39;</code>, which contains a copy of the data originally stored into cell A along with a reference to cell B, which contains a reference to (the original) cell A. Usually compiler optimizations could mitigate the human cost of the change to the “value-based” of model. In pursuit of performance, it still has some impacts on the way when developers are coding, as is explained in the reference:</p>\n<blockquote>\n<p>In other words, the programmer should always act as if the objects themselves were directly manipulated by stack, arithmetic, and other data transformation primitives, and treat the previous discussion only as an explanation of the high efficiency of the stack manipulation primitives.</p>\n</blockquote>\n<!-- -->\n<h2 id=\"Cells-and-Continuations\"><a href=\"#Cells-and-Continuations\" class=\"headerlink\" title=\"Cells and Continuations\"></a>Cells and Continuations</h2><p>“Continuation” is the primitive value type the TVM used to store smart contract code. It represents the “execution token” for TVM, which is vaguely equivalent to a closure in a traditional programming paradigm. </p>\n<!-- TODO -->\n<p>An unexpected virtue of continuation is that it achieves some parallelism. </p>\n<h2 id=\"Conclusions\"><a href=\"#Conclusions\" class=\"headerlink\" title=\"Conclusions\"></a>Conclusions</h2><p>In this article we’ve talked about a few areas of the design and the specification of Telegram Virtual Machine. Here are some takeaways we summarized:</p>\n<ul>\n<li>Generally speaking, the TVM adopts the already widely used gas fee schedule as its economic engine for smart contract users. But the TVM specs are more sophisticated than previous effort of building smart contract execution platform such as Ethereum VM, Tron VM, etc. </li>\n<li>Its native supports of serialization/deserialization on persistent storages, execution determinism, efficient memory management all contributes to the adaptability of blockchain.</li>\n<li>We are seeing much more number of native opcodes, a much better encoding scheme, an efficient approach of serialization, etc. We expect those designs will not only reduce the on-chain storage requirement for a program with same functionality, but also will allow TVM to execute more efficiently – hence less cost.<!-- perhaps talks about semantics? -->\nOverall, we are glad that we see the TVM bringing the blockchain-based smart contract execution environment design to a new level. Smart contracts will execute more safely and more efficiently, with native support from the platform. We hope the nascence of TVM will bring acceleration to the development of decentralized applications.</li>\n</ul>\n","categories":[],"tags":["Compiler","Blockchain","English"]},{"title":"On \"Canonical Transaction Ordering\"","url":"lialan.github.io/2018/11/11/ctor/","content":"<h2 id=\"BTC-BCH区块扩容问题\"><a href=\"#BTC-BCH区块扩容问题\" class=\"headerlink\" title=\"BTC/BCH区块扩容问题\"></a>BTC/BCH区块扩容问题</h2><p>关于区块扩容，有很多争议。</p>\n<ol>\n<li>增加块大小会降低区块空间的稀缺性，降低交易竞争激烈程度，进而降低交易费用。这显然是矿工不愿意看到的。</li>\n<li>增加块大小会增加区块节点网络和存储的负担，减少参与的节点数量，进而减少已有的共识。</li>\n</ol>\n<p>反对者关于区块扩容最重要的一点论据是：Nakamoto Consensus是无法持续扩容的。我们可以从1MB升级成2MB，然后4MB，8MB。<strong>TODO: fill in the gap.</strong></p>\n<h3 id=\"区块的传播\"><a href=\"#区块的传播\" class=\"headerlink\" title=\"区块的传播\"></a>区块的传播</h3><p>假定两个矿工在同一时间各自产出了一个区块，那么决定网络最终会选择哪一个产出的区块，取决于区块在网络的传播速度和网络的拓补结构。所以当一个矿工挖出一个区块之后，他/她当然想以最快的速度将区块传播出去。</p>\n<p>在现有的P2P框架中，一个验证节点在将一个收到的区块转播出去之前需要验证输入区块的正确性。在验证一个区块是否正确之前，验证节点需要完整接收整个区块的内容。留给验证节点产出下一个区块的平均时间，等于10分钟减去从开始接收区块到验证区块结束所需要的时候。所以，无论是对于产出了区块的节点，还是对于接收到新区块的节点，都希望能够将验证节点的时间缩短。</p>\n<p>我们找到了比特币网络扩容的瓶颈之一。</p>\n<p>开发者们提出了很多种扩容的方案。比如Dietcoin<a href=\"https://github.com/bitcoin/bips/blob/master/bip-0035.mediawiki\" target=\"_blank\" rel=\"noopener\">3</a>。</p>\n<h3 id=\"“Topological-Transaction-Ordering”\"><a href=\"#“Topological-Transaction-Ordering”\" class=\"headerlink\" title=\"“Topological Transaction Ordering”\"></a>“Topological Transaction Ordering”</h3><p>基于UTXO的记帐方式相对于基于帐本的记帐方式有更多的并行性。然而这种并行性并不存在于单个帐户内部。比如说A的帐号有一个比特币，用户同时产生了两笔转出交易至B，C，每笔交易金额都为一个比特币。那么最终B，C谁会获得这个这个比特币是由两笔交易上链的顺序决定的（事实上第二笔交易会是无效的）。另外，我们知道，在一个有效的区块中，交易UTXO的排列构成一个偏序结构。软件规定，在一个区块中，假如同时存在一个A->B的一个UTXO，和一个B->C的UTXO，那么在一个区块里面第一个UTXO必须要出现在第二个UTXO的前面。因此在一个区块之内，各个交易存在偏序关系。保持这种偏序关系的排列叫做”交易的拓补顺序”。</p>\n<p>同时，Merkle Root的计算规则与各叶子节点的顺序有关。<strong>TODO:再细讲原因</strong>Merkle Root的计算规则要求<strong>TODO</strong>。故同一组节点以不同的顺序排放会产生不同的Merkle Root的值。因此我们不但需要知道所有的交易，还需要知道交易的顺序，才能验证一个块是否是合法。</p>\n<h2 id=\"优化\"><a href=\"#优化\" class=\"headerlink\" title=\"优化\"></a>优化</h2><h3 id=\"内存池和IBLT\"><a href=\"#内存池和IBLT\" class=\"headerlink\" title=\"内存池和IBLT\"></a>内存池和IBLT</h3><p>各个节点内部均维护着一个内存池(mempool, <a href=\"https://github.com/bitcoin/bips/blob/master/bip-0035.mediawiki\" target=\"_blank\" rel=\"noopener\">BIP035</a>)。当一笔交易被传到网络上后，它会首先被传播到一个验证节点上，存放在内存池中，然后再由当前节点传播转发至相邻节点。由此，研究者发现这种传播的一个特点：相邻节点的内存池中所存放的待打包交易有很大的重合。在这个前提下，相邻两个节点在传输一个刚挖出来的块的时候，大部分时间是在传输对方已经有的交易内容。如果只传输两个节点间不同的部分，就可以大大减少节点之间的通信。</p>\n<p>但是，区块内交易的顺序信息也需要被传输从而被验证。面这个排列的顺序，是节点在符合偏序排列的前提下决定的。因为这个原因，我们不能够仅仅传输与对方不重合的交易内容，我们还需要传输这个由矿工节点所决定的交易排列。</p>\n<h3 id=\"”Canonical-Transaction-Ordering”\"><a href=\"#”Canonical-Transaction-Ordering”\" class=\"headerlink\" title=\"”Canonical Transaction Ordering”\"></a>”Canonical Transaction Ordering”</h3><p>注意到</p>\n<h2 id=\"优势\"><a href=\"#优势\" class=\"headerlink\" title=\"优势\"></a>优势</h2><h2 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h2><p><a href=\"https://github.com/bitcoin/bips/blob/master/bip-0035.mediawiki\" target=\"_blank\" rel=\"noopener\">1</a> Joannes Vermorel and Amaury Séchet, “Canonical Transaction Ordering for Bitcoin,” n.d., 8.<br><a href=\"https://github.com/bitcoin/bips/blob/master/bip-0035.mediawiki\" target=\"_blank\" rel=\"noopener\">2</a> Michael T. Goodrich and Michael Mitzenmacher, “Invertible Bloom Lookup Tables,” ArXiv:1101.2245 <a href>Cs</a>, January 11, 2011, <a href=\"http://arxiv.org/abs/1101.2245\" target=\"_blank\" rel=\"noopener\">http://arxiv.org/abs/1101.2245</a>.</p>\n","categories":[],"tags":["Blockchain","中文"]},{"title":"交易费用经济学（二）","url":"lialan.github.io/2018/09/03/transaction-economics-2/","content":"<h2 id=\"效用函数和共识成本\"><a href=\"#效用函数和共识成本\" class=\"headerlink\" title=\"效用函数和共识成本\"></a>效用函数和共识成本</h2><p>区块链作为一个在无信任状态下的信任网络，它所产生的功用是所有参与节点的共识。当全网参与节点的数目越多，网络的信任共识就越强。因此，我们可以说在一个区块链网络里面，网络的可信程度和网络的节点数量成正相关关系。</p>\n<p>不过一个网络节点要参与到网络中去并不是没有成本的。一个共识网络达成的共识越高（节点数越多），网络的维护成本也相应地越高。节点要付出带宽，计算能力和存储能力。因此可以知道：参与的成本越高，参与节点的数量就越少。按如上的定义，如果一个区块链网络想要有足够大的效用，那么它不但要给各个节点带来足够多的利益之外，还不能让它们的参与成本过高。</p>\n<p><img src=\"/media/Screen%20Shot%202018-09-02%20at%2011.13.29%20AM.png\" alt=\"Screen Shot 2018-09-02 at 11.13.29 A\"></p>\n<p>上图摘自Vitalik的presentation，来源不明。可以看到研究人员做的统计：当区块的容积增大时与全网存留节点的比例关系。当区块容积达到38MB时，全网仅会有一半的节点留存；当达到211MB的时候，仅能剩余10%的节点。也就意味着网络的效用变得很低，网络相比变得非常中心化了。</p>\n<h2 id=\"交易费和存储的关系\"><a href=\"#交易费和存储的关系\" class=\"headerlink\" title=\"交易费和存储的关系\"></a>交易费和存储的关系</h2><p><img src=\"/media/15358535839532.jpg\" alt=\"-w717\"></p>\n<p>上图（来源：<a href=\"http://bc.daniel.net.nz/\" target=\"_blank\" rel=\"noopener\">http://bc.daniel.net.nz/</a>）显示了以太坊节点和比特币节点的存储空间消耗。可以看到，以太坊的存储空间增长率在不断增加。相对地，比特币仍然保持着一个线性地增长。要维护一个以太坊的全节点，在2018年中时所需要的存储空间已经超过了1TB。以太坊这种增长的速度在与节点的存储和带宽的军备竞赛中会导致越来越多的节点被迫退出网络，全网的全节点数量会越来越少。</p>\n<p>毫无疑问以太坊的存储设计出了问题，才导致了网络存储不受限制的增长。如果单纯地为了减少交易费用而增加区块容积大小只会更快地将计算存储和带宽能力较差的节点排除在网络之外，反而会有可能降低整个网络的总体效用。</p>\n<p>以太坊的交易费用设计仅仅考虑了将数据（交易，合约代码等）放到最新出的区块上的成本，并没有考虑该数据永久性的存储在将来所带来的外部性，包括现有的和新的节点在将来的存储成本。假如以太坊存在一百年，光是存储就会带来非常大的外部性。相应地，交易费用因为没有涵盖现在和将来所产生的外部性，所以被大大地低估了。如果不改变现状，维护以太坊的成本将会越来越高，节点数目越来越少，网络共识越来越低。</p>\n<h2 id=\"策略选择\"><a href=\"#策略选择\" class=\"headerlink\" title=\"策略选择\"></a>策略选择</h2><p>要解决这种社会成本溢出问题，需要将交易费用调整至可以涵盖产生的外部性。如何调整可以达到整体的最优解是一个传统的经济学问题。回顾文章的第一部分，我们提到对外部性的调节可以有如下两种方式：</p>\n<ol>\n<li>通过征收Pigouvian Tax（庇古税）来控制价格</li>\n<li>通过限额度交易（Cap-and-trade）来控制资源总量</li>\n</ol>\n<p>Weitzman在他的”Prices vs. Quantities”[1]里面提及：在一个信息完全的环境下，控制价格和控制资源的结果是一致的。但是在现实情况下是信息是不完全的，在这时就会有一个孰优孰劣的选择：在非信息完全的环境下，最佳决策取决于社会成本和私人利益的边际增长率。Weitzman的文章最后也提出，最好的选择办法可能是两者兼顾。真正的最佳决策往往不是单一的。</p>\n<p><img src=\"/media/Screen%20Shot%202018-09-02%20at%203.14.49%20PM.png\" alt=\"Screen Shot 2018-09-02 at 3.14.49 P\"><br>上图摘自Vitalik的presentation。<br>图(a)中<em>绿线</em>代表的实际私人利益曲线，<em>红线</em>代表的社会成本估算值，<em>蓝线</em>代表私人利益估算值。系统信息不完全时的决策由<em>红线</em>和<em>蓝线</em>所代表两条曲线相交处决定。意识到我们离最优点（图a中所示）仍然是差距，逼进的方式是选择控制价格还是控制总量，哪个更优，决定于图b中两个正方形交叉点与最优点的距离。注意到这里仅考虑了私人利益这个变量。</p>\n<p>在下一篇文章里面我们会继续聊如何在区块链这个上下文中确定最佳方案。</p>\n<h2 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h2><ol>\n<li>Martin L. Weitzman, “Prices vs. Quantities,” The Review of Economic Studies 41, no. 4 (October 1974): 477, <a href=\"https://doi.org/10.2307/2296698\" target=\"_blank\" rel=\"noopener\">https://doi.org/10.2307/2296698</a>.</li>\n<li>Vitalik Buterin, “Transaction Fee Economics”, <a href=\"https://vitalik.ca/files/Transaction%20fee%20economics.pdf\" target=\"_blank\" rel=\"noopener\">https://vitalik.ca/files/Transaction%20fee%20economics.pdf</a></li>\n</ol>\n","categories":[],"tags":["Blockchain","中文","Economics"]},{"title":"交易费用经济学（一）-- 基础","url":"lialan.github.io/2018/08/21/transaction-fee-economics/","content":"<p>注：本文参考了Vitalik Buterin<a href=\"https://vitalik.ca/files/Transaction%20fee%20economics.pdf\" target=\"_blank\" rel=\"noopener\">关于交易费的演讲</a>的思路和内容。</p>\n<hr>\n<h1 id=\"公共资源困境\"><a href=\"#公共资源困境\" class=\"headerlink\" title=\"公共资源困境\"></a>公共资源困境</h1><p>1968年，美国学者哈定（Garrett Hardin）发表了著名的文章<a href=\"http://science.sciencemag.org/content/sci/162/3859/1243.full.pdf\" target=\"_blank\" rel=\"noopener\">《公地悲剧》(“The Tragedy of the Commons”)</a>。他用博弈理论讲述了这样一个故事（引用Wikipedia中关于<a href=\"https://zh.wikipedia.org/wiki/%E5%85%AC%E5%9C%B0%E6%82%B2%E5%8A%87\" target=\"_blank\" rel=\"noopener\">公地悲剧的介绍</a>）：</p>\n<blockquote>\n<p>公共草地上，有一群牧羊人，每一個牧羊人都想要多獲利一些，所以某個牧羊人就帶了大量的羊來放牧，雖然他知道過度放牧，草地可能會承受不住。但他依然獲利了，而後所有的牧羊人都跟進，所以草地牧草耗竭，悲劇因而發生了。<br>…<br>他举出一个牧羊人与牧场资源的假设性例子以解释他的论点：牧羊人应该极大化他的牧场，并且尽可能的增加他的羊群。而每增加一头羊，皆会带来正面与负面的影响：</p>\n</blockquote>\n<blockquote>\n<p>正面：牧羊人可以从增加的羊只上获得所有的利润<br>负面：牧场的承载力因为额外增加的羊只有所耗损</p>\n</blockquote>\n<blockquote>\n<p>然而，牧场理论的关键性在于这两者的代价并非平等：牧羊人获得所有的利益，但是资源的亏损却是转嫁到所有牧羊人的身上。因此，就理性观点考量，每一位牧羊人势必会衡量如此的效用，进而增加一头头的羊只。但是当所有的牧羊人皆做出如此的结论，并且无限制的放牧时，牧场负载力的耗损将是必然的后果。于是每一个个体依照理性反应所做出的决定将会相同，毕竟获得的利益将永远大于利益的耗损。而无限制的放牧所导致的损失便是外部性的一个例子。</p>\n</blockquote>\n<blockquote>\n<p>由于这样的个体行为是可预见的，并且将持续发生，因此Hardin称之为“悲剧”：“持续进行，永无休止的悲剧”。从哈丁的假设出发，可以发现追求自我利益的行动并不会促进公共利益。</p>\n</blockquote>\n<p>Hardin如是说：</p>\n<blockquote>\n<p>“Ruin is the destination toward which all men rush, each pursuing his own best interest in a society that believes in the freedom of the commons. Freedom in a commons brings ruin to all.”<br>（”当所有人都相信公地（资源）是自由的，而不断地为个人利益最大化而争抢资源时，毁灭是我们的终点。资源的自由带来的是毁灭。”）</p>\n</blockquote>\n<p>究其原因，是个体不受限制地根据自身利益最大化原则使用公有资源时会产生负的外部性。个体的经济活动忽视了总体社会成本的提高，因此它对整个社会的影响就不再是零和游戏。所以，对公众无限制地开放公有资源造成的会是对资源的滥用。要解决这个公地悲剧，我们必须要想办法抵销使用区块资源时所产生的负的外部性的影响。</p>\n<h1 id=\"交易费的历史\"><a href=\"#交易费的历史\" class=\"headerlink\" title=\"交易费的历史\"></a>交易费的历史</h1><p>中本聪是第一个考虑在区块链上加入一个交易费用的人。2008年底James Donald在质疑比特币的经济模型时提出，比特币要解决的一个经济问题是通胀，然而在一个p2p记帐网络中如果不解决记帐的费用来源问题，网络是没有经济模型能够自主运行起来的。但是如果记帐的费用是由挖矿（seigniorage）来支付的话，整个系统又会需要有通胀。从宏观角度来看，比特币网络的运转需要内在的刺激。于是中本聪提出了在网络中增加用户<a href=\"https://satoshi.nakamotoinstitute.org/emails/cryptography/9/\" target=\"_blank\" rel=\"noopener\">向挖矿者提供转帐费用的激励</a>来解决这个问题。中本聪认为，在比特币网络分发了足够多的比特币之后，网络会完全由交易费用维持，并能够保持无通胀地运行。</p>\n<p>交易费除了可以激励经济体自我运转，它的另外一个作用是防止经济攻击。从微观的角度来看，增加支付交易费使得每个用户使用区块链资源的成本从极低上升到不可忽视，进而限制了用户影响公共资源的能力。于是，恶意用户试图通过大量占用公共资源进行denial-of-service攻击的成本也就大大提高了。比如说，最近NEO网络就受到了垃圾攻击(spam attack)的影响。为了激励用户使用，NEO规定”<a href=\"http://docs.neo.org/zh-cn/sc/systemfees.html\" target=\"_blank\" rel=\"noopener\">目前转账交易没有手续费（每个区块仅限21个交易）。…每个智能合约在每次执行过程中有10 GAS 的免费额度…</a>“。于是，<a href=\"https://neonewstoday.com/general/neo-blockchain-experiences-transaction-spam-attack/\" target=\"_blank\" rel=\"noopener\">这个经济模型漏洞就被人利用了</a>，迫使<a href=\"https://medium.com/neo-smart-economy/annocements-about-attack-transactions-in-neo-9fd7409fb3c6\" target=\"_blank\" rel=\"noopener\">NEO委员会不得不提出改变交易手续费规则</a>。再比如说，如果矿工收不到手续费，他们就可能不愿意去打包传进来的交易请求。这使得矿工会随意产出不带交易的块，甚至放入无意义的交易充斥区块空间。也就是说，交易费既能够吸引矿工节点参与网络建设，又能够防止他们对网络进行攻击。</p>\n<p>当一笔交易被矿工记录在由矿工生成的区块上之后，这笔交易会被广播至全网的其它节点。于是我们发现，虽然全网的节点都承担了记录交易的责任，但是只是挖出区块的矿工获得了交易的手续费。不仅如此，不但这笔被记录了的交易增加了节点当下的存储负担，它也增加了节点将来的存储负担–节点要永久保存这个记录。甚至，新加入的节点也需要承担这个记录的责任。也就是说，每一笔交易对全网节点的现在和将来都产生了交易之外的社会成本，而这个成本并没有被价格所展现。</p>\n<h1 id=\"限额交易和庇古税\"><a href=\"#限额交易和庇古税\" class=\"headerlink\" title=\"限额交易和庇古税\"></a>限额交易和庇古税</h1><p>在发现这个问题之后，Vitalik很自然地提出了一个经济学问题：对于每笔交易所产生的外部性（未承担的社会成本），如何使之内部化？即如何让这个交易产生的额外社会成本变成私人的成本，使内外的影响达到平衡？要解决将未考虑的额外社会的成本加入系统中来的问题，可以有两种实现方式。</p>\n<p>第一种方法是限额交易（cap-and-trade）。即给区块链的资源使用设一个上限，让资源活消耗的增加跟上软硬件基础设施性能的增长。在现实世界中，实施的办法就是规定难度自动调整，约每10分钟产生一个新块，每个块的大小不超过1MB。</p>\n<p>第二种方法是引入庇古税（Pigouvian Tax）。庇古税解决外部性的方法是对产生外部性主体的经济行为征税。在区块链的上下文里，交易手续费即是一种庇古税。但是，仅向矿工支付交易手续费并不是一个很好的税制，甚至谈不上是庇古税：首先交易费没有损耗，矿工可以和交易者合谋，产生新的经济攻击方式（我们会在下一篇文章里讨论）；其次，交易费的受益者只有矿工一个人，还缺少一个调节全网成本的机制。</p>\n<hr>\n<p>在下一章我们会讨论Vitalik会如何设计交易费机制，以及以太坊的其它问题。</p>\n","categories":[],"tags":["Blockchain","中文","Economics"]},{"title":"Project NuCypher -- its staking economics","url":"lialan.github.io/2018/08/16/nucypher/","content":"<p>本文首先介绍了NuCypher项目的基本情况，以及项目的挖矿机制。文章的第二部分讨论了staking这种挖矿机制方案的设计，用途等。</p>\n<hr>\n<h1 id=\"NuCypher的介绍\"><a href=\"#NuCypher的介绍\" class=\"headerlink\" title=\"NuCypher的介绍\"></a>NuCypher的介绍</h1><h2 id=\"项目简介\"><a href=\"#项目简介\" class=\"headerlink\" title=\"项目简介\"></a>项目简介</h2><blockquote>\n<p>NuCypher KMS是一个分布式密钥管理系统(KMS)，提供基于加密和密码学的权限控制服务。不像中心化的KMS提供的服务，它不需要信任服务提供商，并且它能够在公共网络上任意数量的参与者之间共享私有数据，使用代理重加密技术来代理解密权限，这是传统对称加密和公钥加密方案不能实现的。</p>\n</blockquote>\n<blockquote>\n<p>传统上，中心化KMS服务商有：亚马逊云HSM，谷歌云KMS，微软Azure Key Vault和TrueVault等，在使用这些应用时，需要对这些中心服务提供商设置不适当的信任级别，这对于安全性至关重要的应用程序来说可能不合适，而且容易受到单节点攻击。（比如之前币安受到黑客的攻击就属于节点攻击）</p>\n</blockquote>\n<blockquote>\n<p>NuCypher KMS 使用分布式网络移除对中心化服务提供商的信任，使用代理重加密提供密码访问控制，使用代币激励机制保证可靠性，可用性和正确性。由于使用代理重新加密，未加密的对称密钥（能够解密私有数据）绝不会暴露在服务器端，即使被攻破，黑客也只能得到重新加密的密钥，并且对文件的访问仍然受到保护。</p>\n</blockquote>\n<blockquote>\n<p>应用场景包括：共享加密文件（分布式Dropbox）、端到端加密群聊（加密Slack）、患者控制的电子健康记录（EHR）、分布式数字版权管（DDRM）、盲身份管理、共享凭证和企业密码管理等等。[3]</p>\n</blockquote>\n<p>NuCypher所提供的是一种基于p2p网络的分布式密钥管理服务。与其它系统相比，它不需要一个中心化的服务提供商。为用户加密数据的是p2p网络中的其它用户。这种服务有一个特点，即用户的行为是将自己的数据加密服务交予某些节点运行。那么在用户每次使用服务的时候，网络都应该尽可能保证有能够节点为该用户提供服务，否则网络是不可靠的。</p>\n<h2 id=\"挖矿协议简介\"><a href=\"#挖矿协议简介\" class=\"headerlink\" title=\"挖矿协议简介\"></a>挖矿协议简介</h2><p>团队采用了staking（质押）方式激励用户参与网络。即：</p>\n<ol>\n<li>参与节点先用智能合约锁定一定量的代币，并声明在一定的时间内向全网络提供可靠的服务</li>\n<li>若用户如约在声明的时间内履约，提供并完成了可靠的网络服务，则获得相应的回报</li>\n<li>若用户违约，则需要扣除一定的锁定代币</li>\n</ol>\n<p>在设计激励机制的时候，团队做了如下考虑[2]:</p>\n<ul>\n<li>回报应该与投入成正比例关系</li>\n<li>对于staker来说，投入的stake应与该节点承载的工作量成正比例关系</li>\n<li>网络应该激励staker，让他们更长时间地参与</li>\n<li>主网上线的前期的高通胀对币价的影响应该受控制，使得更多的stakers参与进来</li>\n<li>网络应该激励stakers让他们长期在线</li>\n<li>网络应该对re-staking的行为进行激励，即鼓励stakers在获得staking的回报之后将回报锁定</li>\n<li>网络也应该对短期的staking激励，但是回报率要低一些。</li>\n</ul>\n<p>故针对基本的staking，挖矿协议的内容亦增加了一些：</p>\n<ol start=\"4\">\n<li>对于restaking(即参与staking的节点在完成之后，将本金和利润都全部投入下一期的合约锁定中)，网络将提供更高的回报支持</li>\n<li>考虑到网络和硬件本身就存在不稳定的问题，如服务器或者程序升级等造成的短期服务中断都是常见的。如果一个节点在锁定的过程里有失效的时间，本金将仍然会被锁，直到节点履行了义务为止（如完成了指定的上线时间等）。</li>\n</ol>\n<p>综上所述，NuCypher提出的staking的激励分配方案主要用于解决现有虚拟货币的分配机制中：<br><em>. 激励节点在线向用户提供服务，以及\n</em>. 上线之后长期和短期内出现的通货膨胀问题。</p>\n<hr>\n<h1 id=\"讨论\"><a href=\"#讨论\" class=\"headerlink\" title=\"讨论\"></a>讨论</h1><h2 id=\"挖矿机制的设计\"><a href=\"#挖矿机制的设计\" class=\"headerlink\" title=\"挖矿机制的设计\"></a>挖矿机制的设计</h2><p>在关于挖矿的分发机制的设计上，团队首先设定了整体的挖矿框架，通常而言挖矿机制有如下几种大类可以供选择：</p>\n<ol>\n<li>没有挖矿环节，分发是确定的。如众多的基于ERC20协议开发的不增发的代币。</li>\n<li>前期高通胀PoW挖矿，以一定比例衰减。如比特币。</li>\n<li>在一定时间内以一个稳定的速率通胀。如Zilliqa：他们希望挖矿代币的分发在5年内完成，而在5年后网络能不再需要产生新的代币，网络只由转帐的gas费用维持，达到一个动态平衡。</li>\n<li>Proof-of-stake</li>\n<li>混合机制，如PoW和PoS共存</li>\n</ol>\n<p>NuCypher选择了基于经典的初期高收益的挖矿通胀回报早期参与者的挖矿方案，加上proof-of-stake。基于这个大的框架，该项目关于挖矿的设计[2]讨论了在满足以上的需求时，通帐参数的选择问题。需要说明的是，这种讨论的前提是基于PoW加上PoS的的框架而产生的，基于这个前提缩小了通证经济学讨论的范围，并不能够普遍适用于另外几种方案。</p>\n<h2 id=\"通胀分析\"><a href=\"#通胀分析\" class=\"headerlink\" title=\"通胀分析\"></a>通胀分析</h2><p>在[2]中，NuCypher团队的设计者将网络的生命周期以系统上线分初始阶段和后续两个阶段来考虑。原因是在这两个阶段里，有不同的经济模型：前期的高通胀用以激励矿工的参与，而在长远的将来用户数量增加至矿工网络需要通过收取服务费来维持的时候，则需要降低系统的通胀（激励）比例，维持一个动态平衡。另外，两个阶段之间的过渡没有明显的界限。在两个阶段之间币价如何能够有一个平稳的衔接过渡也是一个要讨论的问题。</p>\n<p>在项目的早期就过早地失去矿工社群是每个项目都不希望看到的事情，因此项目前期的激励是非常重要的。但是，上线初期的流通非常小，会容易产生很剧烈的通胀。以ZCash举例来说，项目上线的前几个月内年化率达到了360%，这使得在项目上线后的前几个月跌破了初始价。随着用户社群增加，后来币值才恢复并不断一路飙升。于是，Proof-of-Stake在这里就很适用，它不但要求质押保证金，提高矿工节点放弃之前的服务承诺的成本，还能够锁定币值，减少流通量，减少通胀。一举两得。</p>\n<p>对于比特币而言，代币的供应量试图模拟黄金这样的硬通货的开采速度[9]。对于NuCypher，团队用指数衰减来使得在每一时刻$t$,货币的增量数与仍然未挖出的货币数成正比。用一个统一的代币供应函数去拟合两个不同的系统经济阶段。</p>\n<h2 id=\"矿工的挖矿策略\"><a href=\"#矿工的挖矿策略\" class=\"headerlink\" title=\"矿工的挖矿策略\"></a>矿工的挖矿策略</h2><p>讨论到这里还并不够区分度。如果单一地对staking规定回报力度和方式，则无法对长期和短期锁定的激励有足够的区分。</p>\n<p>下图描述了为激励矿工长期锁定，对矿工锁定期限长短给予的补偿比例[2]。<br><img src=\"/media/15349929828102.jpg\" alt=\"-w649\"><br>可以看到，NuCypher以一年为界限，对于锁定期一年以下的矿工减少补偿，最低可达基准补偿的50%。根据这个曲线我们可以观察到，一个矿工总会是以一年为界限锁定NKMS。</p>\n<p>对于矿工，一般而言会有以下三种挖矿行为：</p>\n<ol>\n<li>在staking周期结束之后，带着收益和本金离场</li>\n<li>在staking周期结束之后，将收益和本金继续质押(restaking)</li>\n<li>每一次staking的时间越来越短(spin down)</li>\n</ol>\n<p>根据不同的需求，矿工有足够的自由度选择适合自己的挖矿策略。对系统而言，代币增发的比例上界就可以在初期确定–上限就是所有的矿工都选择承诺至少一年的锁定期限。当长期锁定的矿工数量减少，整体的代币增发比例也随之减少。</p>\n<p>这样产生了一个动态计算的通胀平衡：如果NuCypher网络用户比较少，币价下跌，则矿工会减少，随之通胀也会减少，抑制币价继续下跌的速度；如果网络用户增多，币值增加，则矿工会增加，随之通胀也会增加，减缓币价波动。</p>\n<h2 id=\"Staking可以适用的应用场景\"><a href=\"#Staking可以适用的应用场景\" class=\"headerlink\" title=\"Staking可以适用的应用场景\"></a>Staking可以适用的应用场景</h2><p>对于需要节点提供availability的项目，可以考虑staking的设计。比如：</p>\n<ol>\n<li>基于闪电网络相关的服务。在闪电网络中，用户之间的每次交易都需要由闪电网络的节点记帐，故需要激励记帐节点。</li>\n<li>基于去中心化的文件存储或者数据库的服务。数据存储项目天然地强调high availability。</li>\n<li>强调节点治理(governance)的项目。代码不是万能的，对于需要人工参与决策，如投票，的内容，需要以某种方式激励委员会的参与度。</li>\n<li>等等等等….</li>\n</ol>\n<p>NuCypher提出的基于PoS的激励方式，寄希望于在解决经济激励问题的同时，能够设计得减少币值的剧烈波动问题。需要说明的是文中讨论的NuCypher所设计的staking的经济激励模式并不普遍适用于所有的区块链项目。在一个不需要强调用户节点在线时间的项目中（如bitcoin)是不应该使用这种复杂的奖励机制的–它会使得项目的设计变得过于复杂。</p>\n<h2 id=\"挑战者的角色\"><a href=\"#挑战者的角色\" class=\"headerlink\" title=\"挑战者的角色\"></a>挑战者的角色</h2><p>NuCypher的白皮书[8]中提到，在一个需要持续提供服务支持的p2p环境下，挑战者是很重要的参与者。过少的挑战者会降低作恶的机会成本，给网络中服务提供者提供作恶的动机和可能性。区块链p2p网络中需要有相应的角色来保证参与的节点的可靠性。TrueBit[5]提出了一种中奖的机制鼓励社区参与监督网络环境的运行。其具体的内容可以参考[6]以及我们之前的讨论[7]。在NuCypher这样的类似的区块链服务框架下，也应该需要相应的挑战者的角色来确保矿工节点不存在怠工问题。NuCypher的白皮书中加入了挑战者的角色，不过它所描述的挑战者的作用是找到“泄漏了一个重加密密钥”的矿工。</p>\n<p>由此我们可以考虑，对一些需要提供availability的项目，除了staking的机制之外，挑战者的角色也可以在经济层面上作为一个监督矿工节点拒绝服务的防范。如在IPFS的经济设计中，仅当读取存储在p2p网络上的文件时用户是需要付费的，而存储不需要付费。这很难保证存储信息在一定时间内的availability问题。Staking和挑战者角色的加入，可能可以改变这种困境。</p>\n<h2 id=\"Staking模式的适用性\"><a href=\"#Staking模式的适用性\" class=\"headerlink\" title=\"Staking模式的适用性\"></a>Staking模式的适用性</h2><p>另外，在staking的细节上，也有可以变化的地方。比如在NuCypher中，我们很难去记录一个矿工在线的时间。但是，我们可以很容易地记录一个矿工所完成的工作量。假如说质押的要求不是在线时间，而是工作量的话，也许会让设计变得更容易一些。但是NuCypher团队并没有选择这样的方案，可能是难以界定单个工作量的大小等。不过，这给了我们一种对staking机制设计多样化的一种启发，可以在以后的设计中使用除时间之外的不同的衡量标准。</p>\n<h2 id=\"综述\"><a href=\"#综述\" class=\"headerlink\" title=\"综述\"></a>综述</h2><p>NuCypher的团队认为激励机制，和共识机制，以及治理机制一样是区块链中最重要的三个部分，因此花了很多时间设计相应的经济模型。因为NuCypher的代币还在设计中，还没有测试网络可以公开使用，我们无法知道实际运行的效果是偏向于哪一方面。但是我们在上线初期根据整体代币的锁仓比例，长期锁仓和短期锁仓的矿工的比例，就能较为准确地预测到一年之前的系统通胀比例。这可以作为二级市场投资和一级市场退出时间点选择做一个基本面的分析。</p>\n<hr>\n<h1 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h1><ul>\n<li>[1] MacLane Wilkison, “NuCypher Staking Economics,” NuCypher, August 5, 2018, <a href=\"https://blog.nucypher.com/nucypher-staking-economics-a7bb56b20716\" target=\"_blank\" rel=\"noopener\">https://blog.nucypher.com/nucypher-staking-economics-a7bb56b20716</a>.</li>\n<li>[2] Michael Egorov and MacLane Wilkison, “NuCypher: Mining &amp; Staking Economics,” n.d., 6.</li>\n<li>[3] NuCypher 投资项目测评， <a href=\"https://kuaibao.qq.com/s/20180319G0OBUS00\" target=\"_blank\" rel=\"noopener\">https://kuaibao.qq.com/s/20180319G0OBUS00</a></li>\n<li>[4] NuCypher Contract, <a href=\"https://github.com/nucypher/nucypher/blob/master/doc/contracts.rst\" target=\"_blank\" rel=\"noopener\">https://github.com/nucypher/nucypher/blob/master/doc/contracts.rst</a></li>\n<li>[5] Jason Teutsch and Christian Reitwießner, “A Scalable Veriﬁcation Solution for Blockchains,” n.d., 50.</li>\n<li>[6] Truebit白皮书解读, <a href=\"https://zhuanlan.zhihu.com/p/36487861\" target=\"_blank\" rel=\"noopener\">https://zhuanlan.zhihu.com/p/36487861</a></li>\n<li>[7] 验证者困境和TrueBit的解决方案, <a href=\"https://zhuanlan.zhihu.com/p/38582224\" target=\"_blank\" rel=\"noopener\">https://zhuanlan.zhihu.com/p/38582224</a></li>\n<li>[8] Michael Egorov, “NuCypher: A Proxy Re-Encryption Network to Empower Privacy in Decentralized Systems,” n.d., 23.</li>\n<li>[9] Bitcointalk Wiki, <a href=\"https://en.bitcoin.it/wiki/Controlled_supply#Currency_with_Finite_Supply\" target=\"_blank\" rel=\"noopener\">https://en.bitcoin.it/wiki/Controlled_supply#Currency_with_Finite_Supply</a></li>\n</ul>\n","categories":[],"tags":["Blockchain","中文","Economics"]},{"title":"验证者困境和TrueBit的解决方案","url":"lialan.github.io/2018/06/29/verifiers-dillema/","content":"<p>（此文发布于知乎专栏<a href=\"https://zhuanlan.zhihu.com/p/38582224\" target=\"_blank\" rel=\"noopener\">UNICHAIN技术研究院</a>）</p>\n<hr>\n<h1 id=\"验证者困境-The-verifier’s-dilemma\"><a href=\"#验证者困境-The-verifier’s-dilemma\" class=\"headerlink\" title=\"验证者困境 (The verifier’s dilemma)\"></a>验证者困境 (The verifier’s dilemma)</h1><p>在传统的PoW (Nakamoto Consensus)共识中，矿工为了证明自己的计算能力，需要在确定了区块内的交易内容之后，再寻找一个nonce值，使得它们共同的哈希函数落在某个区间之内。这是一个计算复杂度非常高的操作，以2018年6月的比特币全网算力约40E来计算，相当于约300万台比特大陆S9矿机在同时竞争出块。这也显示了现今竞争PoW计算的困难程度。</p>\n<p>而作为一个验证者节点，它并不需要再次寻找这个nonce值。验证者仅仅需要将新产生的块的nonce值，与块的交易内容合并计算出块的哈希值，再与区块所附带的值作比较，省去了寻找nonce值的过程。对于一个有大小上限的区块协议来说，验算所需要的计算量是恒定的。这样的一个验算操作，即使在手机上甚至都不需要万分之一秒就能完成。</p>\n<p>这是一个非对称式的算力验算证明过程：验算者并不需要很高的算力就可以验证产生区块所使用的算力。另外一个例子是数独。在验证数独问题的解时，我们只需要计算每行每列每个九宫格内的数字是否不重复; 但是在计算数独问题的解的时候，我们需要考虑的是一个搜索范围大很多的问题。</p>\n<p>但是并不是所有的计算问题类型都有非对称的验算方法。智能合约的验算便是一例：图灵完备的计算模型框架下，不存在一个通用的低算力验证的方法。</p>\n<p>以太坊的解决方案是在各个节点上都按交易顺序执行状态机复制（state machine replication）。即智能合约的计算，会在每一个验算节点上重复执行一遍。这样保证了合约的状态的更新，在每个节点上都是一致的，然而这样的设计使得以太坊的智能合约计算成本很高。由于需要在每一个节点上执行智能合约从而验证和更新合约状态，高成本的智能合约验算是以太坊的扩容的一个难点。</p>\n<p>Luu et al. [1]指出，区块链中的验证者存在一个难题：即在现有的区块链经济框架中，验算者验算过程是没有经济激励的。假如验算所需要的算力或时间成本大于一个节点能够承受的值，那么验算者就有作弊的动机：它可以跳过验算而直接承认计算结果;另一方面，假如验算者严格遵守协议的内容进行验算，就有可能受到攻击者的攻击。</p>\n<p>举例来说，假如一个非常复杂的智能合约的执行时间为10秒，如果矿工严格遵守验证规则进行验算计算，那么留给矿工挖矿的时间就会减少10秒。对于一些出块间隔时间较短的区块链协议来说，这种差别可能对矿工来说是致命的。于是，矿工就有可能直接跳过验算的过程，让未验算的区块进入链中。这造成了攻击区块链协议的机会。</p>\n<p>以太坊对timeout的解决方案是引入gas limit机制，以防止验算过程过长。但是这并不能阻止DoS攻击[1]。</p>\n<p>另外，在一个DPoS分布式计算框架下，以4秒为一阶段产生下一个块。那么每个超级节点就需要在这4秒之内：</p>\n<ol>\n<li>对获得的上一个区块进行验算</li>\n<li>打包产生下一个区块</li>\n<li>发送生成的区块给同僚<br>在这里，如果上一个区块包含了一个恶意的智能合约，并且它的计算时间超过4秒，那么如果当前节点严格遵守验证协议的话，就极有可能会失去4秒钟的打包机会，受到攻击。</li>\n</ol>\n<p>这样的攻击问题，在区块链协议的TPS日益增加的情况下，会显得格外明突出。</p>\n<h1 id=\"TrueBit验证\"><a href=\"#TrueBit验证\" class=\"headerlink\" title=\"TrueBit验证\"></a>TrueBit验证</h1><p>TrueBit团队引入了验证者激励机制，试图解决验证者的困境[2]。有趣的是，提出区块链协议中存在验证者困境问题的也是TrueBit团队。</p>\n<p>TrueBit尝试解决以下一个问题：在一个不可信的环境中，如何通过经济激励和相应的协议设计，安全地解决计算扩容问题。如在前一章节中所述，以太坊对于这样一个问题的解决方法，是将合约的计算在网络节点中重放，利用全网的冗余性来保证计算的正确性。</p>\n<p>TrueBit的尝试是从另外一个角度去解决这个扩容问题：计算不再需要在所有的参与节点上重现，而是全网随机推举一个计算者执行相应的计算。另外，网络中存在验证者对结果进行验证，保证计算者的执行是正确的。这样的执行方式类似存在超级节点确权的共识机制。但不同的是，TrueBit的计算和验证方式是去中心化和不可预测的。</p>\n<p>TrueBit的计算过程分为多步。以下是简单的介绍：</p>\n<ol>\n<li>一个成员如果有在链上计算的请求，它会在网络中声明计算任务，并标明计算完成后将获得的奖励。</li>\n<li>通过抽签，全网找到一个计算问题的成员，我们称之为计算者(Solver)。<br>计算者在计算的时候，需要同时准备一个正确的答案和一个错误的答案。<br>另外，还需要将一定量的币，当作stake一起提交。<br>这时候，TrueBit网络会向计算者隐蔽地发出两种不同的请求之一：<ul>\n<li>如果TrueBit网络要求计算者“强制出错”，那么计算者就要把错误的答案发给大家。</li>\n<li>如果网络没有要求计算者“强制出错”，计算者就需要把正确的答案展示出来。</li>\n</ul>\n</li>\n<li>验证者们在得到计算者的答案之后进行计算。<br>如果他们发现在“强制错误”的前提下，计算者提供的答案是错误的，就会获得一笔丰厚的奖金。</li>\n<li>如果验证者们在一定的时间内都没有报告错误，网络就接受这个计算的结果。<br>如果验证者们找到了问题，他们也需要将一定量的币，当作stake连同异议消息一起提交给网络。<br>之后就会进入一个叫做“验证游戏”环节（verification game)。</li>\n</ol>\n<p>在这里面，发出“强制错误”的信息是TrueBit网络的一个特色。全网会每隔一段时间向计算者随机提出一次“强制错误”，并对验证者们隐藏。这样做的目的是为了奖励那些一直在努力进行验证工作的验证者成员们，以保证他们有足够的参与度。“强制错误”同时也对计算请求方和计算者隐藏。</p>\n<p>TrueBit的白皮书[2]中详细，量化地说明了奖金如何而来，数量应该是多少，产生“强制错误”的间隔等因素。有兴趣的读者可以继续阅读他们的白皮书，我们在这里不再详细介绍。</p>\n<p>验证游戏环节是当计算方和验证方有分歧出现时，全网解决计算分岐的方法。它的做法如下：</p>\n<ol>\n<li>计算方和验证方（这里视为挑战者）首先各自对计算有分岐的两个计算结果，划分出c个时间片。</li>\n<li>计算方和验算方同时各自储存下c个时间片末尾的计算中间状态。<br>并产生c个Merkle Tree，表示每个中间状态的状态内容，然后把Merkle Root发到链上。</li>\n<li>挑战者指出第一个存在分岐的位置i，向全网告知。</li>\n<li>裁判首先判断各自均有c个提交的Merkle tree，并且1 &lt;= i &lt;= c。<br>其中如有提交错误信息者立即被判决失去资格。<br>如果没有错误，那么考察第i个时间片的Merkle root是否是不一致的，i-1的merkle root是否是一致。</li>\n<li>既然i是第一个出现分岐的位置，那么，下一步就是缩小分岐。<br>继续在i-1和i之间切割状态，直到找到最小的分岐临界点。</li>\n<li>最后裁判们执行最小的一步计算，确认最终哪一方获胜。<br>获胜方不但可以取回自己押付的stakes，还可以得到失败方的stakes。<br>在规则里体现了TrueBit设计的一个思考：在一个区块链这样的非可信网络上，所有的行为都应该是有成本的。这种成本可以是经济成本，也可以是机会成本。如果没有这样的一种成本，就会降低攻击者的攻击成本 — 这样的协议将会非常容易受到攻击。在TrueBit网络中，押付押金的作法提高了计算者和验证者们的作恶的经济成本，以一种预先防护的方式保护协议不会受到攻击。</li>\n</ol>\n<h1 id=\"对TrueBit验证机制的一些思考\"><a href=\"#对TrueBit验证机制的一些思考\" class=\"headerlink\" title=\"对TrueBit验证机制的一些思考\"></a>对TrueBit验证机制的一些思考</h1><p>TrueBit提出的验证机制提供了解决验证者困境的一种方法。以下是我们对这种机制的一些思考。</p>\n<p>首先，TrueBit使用了一种特制的智能合约虚拟机，它能够记录在某一时间间隔（这我们姑且认为这是可以做到的）处的智能合约虚拟机的执行状态。对于本身计算量非常复杂的智能合约来说，在验证的过程中所需要记录的信息量可以说是非常巨大的。</p>\n<p>在TrueBit的设计中，记录下来每一个中间的状态是为了能够通过二分法在较短时间内找到分叉点，并让众多裁判们直接导入临界状态值，直接计算和判断分叉状态的对错。但是这种想法是不现实的：一个复杂合约的众多中间状态的存储，传输，都会对系统造成不小的压力。</p>\n<p>一个妥协的解决的办法是我们只记录每个状态的哈希值。然后在挑战的时候，首先通过二分法找到第一个出现不同哈希值的位置，然后再让所有裁判从头开始回放执行合约直至分叉处，判别对错。对于这种情况，所需要的验算时间复杂度与智能合约的时间复杂度成正比。</p>\n<p>实际上，白皮书中的计算时间间隔，是一种只在理论上可行的方式。不同的节点在不同的配置下执行速度不会一致。现实中很难做到在程序执行的某个CPU tick上截取程序运行状态。实际可行的方案应该是对在虚拟机上的cycle数进行分段记录。如果为了执行效率的考量选择支持二进制编译语言，如C/C++/Rust等，又实际上增加了快照方案实现的难度，同时也会给节点的安全性带来很大挑战。</p>\n<p>TrueBit的选择是：使用二进制编译代码执行计算任务，当需要解决纠纷时，使用Google Lanai解释器。Lanai架构是一个执行速度非常快的32位，顺序发射处理器架构，这使得Lanai解释器简单高效。TrueBit同时使用两种不同的架构极大地增加了实际问题的复杂性。粗略地思考，有如下一些问题需要解决：</p>\n<p>计算任务和验证任务在不同的指令架构上执行，这要求编译器在生成两种不同架构的代码时能够保持代码相同的语义。这是一件非常困难的事情。<br>C/C++这类系统级语言存在着非常多implementation defined的行为。一致性的需求要求编译过程对这种implementation defined的行为统一化处理。这同样是一件非常困难的事情。<br>解释执行效率远远低于编译执行，这使得进行验证游戏所需要的时间远远高于计算耗时（可以达到数百倍的差别）。<br>此外，验证游戏也对执行环境带来要求：为了保证可实施性，它增加了计算代码的的优化难度。</p>\n<p>再次，因为验算者提出异议是一个主动的过程。当一个包含智能合约的区块被产生之后，需要等待一段时间，让验算者提出异议。由于节点算力的不同，并且需要考虑网络传输延迟，这个等待的时间不能够太短。并且，在这个区块没有被验证的前提下，下一个区块也不能够被产生（即使是我们能够以流水线的形式产生下一个区块，实际上也是增加了被攻击的成本）。也就是说，这种验算方式其实增加了出块确认的延迟时间。</p>\n<p>综上所述，TrueBit提供的解决方案是一次对区块链中的各种作恶行为进行一种约束的尝试。对于一个这样极其复杂的问题，它指明了一个新的方向：在对区块链扩容的过程中，单个节点的固有的验算工作量是无法再降低的，我们需要减少全网的验算成本才能够继续提高区块链协议的容量。另外，在TrueBit出现之前，保持全网正确性的验证者们一直都没有获得足够的经济回报，在验算需求不断增大的情况下，这将会是协议容易被攻击的弱点。需要引入对验证者的经济激励才能保证网络的正确性。</p>\n<h1 id=\"参考文档\"><a href=\"#参考文档\" class=\"headerlink\" title=\"参考文档\"></a>参考文档</h1><p>[1] Luu et al, “Demystifying Incentives in the Consensus Computer“ <a href=\"https://eprint.iacr.org/2015/702.pdf\" target=\"_blank\" rel=\"noopener\">https://eprint.iacr.org/2015/702.pdf</a></p>\n<p>[2] Teutsch et al, “A scalable verification solution for blockchains” <a href=\"https://people.cs.uchicago.edu/~teutsch/papers/truebit.pdf\" target=\"_blank\" rel=\"noopener\">https://people.cs.uchicago.edu/~teutsch/papers/truebit.pdf</a></p>\n","categories":[],"tags":["Blockchain","中文","Economics"]},{"title":"2018 Blockchain Technical Reviews","url":"lialan.github.io/2018/06/29/2018-blockchain-tech-review/","content":"<p>Author: <em>Fuli</em><br>Fuli is a member of the Technical Advisory Committee of FBG Capital<br>Origin: <a href=\"https://zhuanlan.zhihu.com/p/38153345\" target=\"_blank\" rel=\"noopener\">https://zhuanlan.zhihu.com/p/38153345</a><br>Translator: <em>Alan Li</em><br><strong>Disclaimer: This articles is not an investment advice.</strong></p>\n<hr>\n<p>We have observed that, from mid 2017 to early 2018, expanding blockchain scalability has been the hottest area of the blockchain technical development. There are several categories of on-chain scalability expansion solutions:</p>\n<h2 id=\"Sharding\"><a href=\"#Sharding\" class=\"headerlink\" title=\"Sharding\"></a>Sharding</h2><p>Represented by Projects such as Zilliqa, NUS.<br>Competitive consensus only recognizes transactions on the single longest chain, while cooperative consensus mechanism includes more transactions.<br>Sharding gives TPS an approximate linear bump to a certain degree, as it includes otherwise invalid transactions on non-longest chains in a longest-chain architecture.   </p>\n<h2 id=\"DPoS\"><a href=\"#DPoS\" class=\"headerlink\" title=\"DPoS\"></a>DPoS</h2><p>In DPoS consensus blockchains, trusted nodes are elected to take turns to do ledgering, and everybody else just follows.<br>Such technology has a really hot market rate, but suffers from various of technological controversies:</p>\n<ol>\n<li>Risk of being DDoS’d. </li>\n<li>High TPS might lead to so called “<a href=\"https://eprint.iacr.org/2015/702.pdf\" target=\"_blank\" rel=\"noopener\">the verifier’s dilemma</a>”. The dilemma can be described as that miners are not incentivized to verify each other’s blocks as it impacts their own subsequent block producing. This might cause the system to lose its effectiveness.</li>\n<li>Stakes’ early concentration renders subsequent stakeholder voting meaningless. </li>\n</ol>\n<h2 id=\"Sidechain\"><a href=\"#Sidechain\" class=\"headerlink\" title=\"Sidechain\"></a>Sidechain</h2><p>Sidechain projects such as pchain, cosmos, similar to sharding , have a divide-and-conquer philosophical idea towards scalability. Sidechain projects might focus more on heterogeneous blockchain architecture integration, and cross-chain ecosystem building.<br>Concerns about cross-chain interactions:</p>\n<ul>\n<li>finality of cross-chain transaction</li>\n<li>side chain’s temporary forking</li>\n<li>inconsistent block time between main chain and side chains</li>\n<li>main chain has to ensure an irreversible pegged transaction  </li>\n<li>incompatible ledgering system between main and side chains</li>\n</ul>\n<p>Besides those above-mentioned on-chain scalability solutions, off-chain scalability projects are also another market hot spot. Take last year’s Raiden Network for example, the team has shown in the Devcon3 that it is capable of doing off-chain payment in a realistic scenario.  It is through the combination of on-chain <a href=\"https://en.bitcoin.it/wiki/Hashed_Timelock_Contracts\" target=\"_blank\" rel=\"noopener\">HTLC</a> and off-chain channel that the system achieves high-frequency transactions and periodic (such as daily) on-chain settlements.<br>These kinds of HTLC techniques, originally come from Lightning Network, has some intrinsical flaws. Such as the network asks funds be tied into it before use; a persistent states channels/networks infrastructure for payment transactions whenever possible (<a href=\"https://www.trustnodes.com/2018/03/26/lightning-network-economically-broken-says-emin-gun-sirer-%E2%80%8F\" target=\"_blank\" rel=\"noopener\">reference</a>). </p>\n<p>Projects that tries to improve transaction liquidity (Celer, for example) has emerged this year, which is essential for massive adoption for the network. In a sense, Celer is to lightning network what IP/BGP is to HTTP. </p>\n<p>Another excellent off-chain project emerged this year is <a href=\"https://liquidity.network/\" target=\"_blank\" rel=\"noopener\">Liquidity.Network</a>. It is not only a non-custodian off-chain payment network, but also have a simplified networking layer protocol (reference: <a href=\"https://eprint.iacr.org/2017/823.pdf\" target=\"_blank\" rel=\"noopener\">Revive: Rebalancing Off-Blockchain Payment Networks</a>). We are not going to talk about off-chain scalability solution here as we will have dedicated discussion in the future.</p>\n<p>Those off-chain p2p protocol, essentially requires a supporting network as its infrastructure. Payment layer is not a crypto-graphical channel, but a layer of networks.</p>\n<p>On the other hand, off-chain decentralized exchanges protocols also shares a similarity: they requires a relay network to improve transaction liquidity. Such as relayer nodes in 0x protocol.</p>\n<hr>\n<h2 id=\"Privacy-Preserving-Computation\"><a href=\"#Privacy-Preserving-Computation\" class=\"headerlink\" title=\"Privacy-Preserving Computation\"></a>Privacy-Preserving Computation</h2><p>Privacy-preserving computation is another trend in blockchain technologies in 2018. Unlike public blockchain technologies, the competition of privacy-preserving computation is much less intensive.</p>\n<p>There is a enormous gap between data and data processing. Companies that have computing capabilities such as AI and data-processing, craves for all kinds of data. Meanwhile, data owners of economics, health, personal activities etc, thinks the <strong>data is sensitive</strong>. They even do not trust third-party data analyzing institutions, fearing that those sensitive data might be leaked, or used for other commercial purposes, illegal or not.<br>This, forbids discovering true values within data, rendering huge amount of data unused and unanalyzed.<br>If we combine a <strong>trustless</strong>, autonomous computation environment that is not controlled by any entity, and economical incentives driven by token, we could build an autonomous data marketplace powered by AI and privacy preserving computation.</p>\n<p>Decentralized computation outsourcing is hardly a brand-new concept. 2017 projects like Golem, 1protocol have their answers to the outsourcing problem.  Under the premise of <strong>trustless</strong>, decentralized computation outsourcing strategies have to consider the following 2 problems:</p>\n<ol>\n<li>how does a public-known algorithm analyze open, insensitive data</li>\n<li>how does a public-known algorithm analyze sensitive, private data</li>\n</ol>\n<p>Let’s consider the two problems individually.<br>The first problem considers <strong>verifiable computation</strong>: miners who contribute their computing resources, have to prove themselves that they have indeed completed the outsourced computation task — without cheating by providing forged computing results in order to save costs. Miners have a strong incentive to cheat.</p>\n<p><strong>Verification is at the heart of the problem</strong>. Fundamentally, if <strong>the cost of verifying == the cost of computing</strong>, the outsourcers would have no incentives to distribute computing tasks to a decentralized network — it is cheaper to do by their own.<br> Naturally, verifiers could challenge miners with known, random results (<strong>known-result challenge</strong>).  Once a framing miner is found, and proof is submitted, the convicted will be punished economically.<br>A more advanced method is to introduce <strong>validator</strong>s who, randomly sample and verify outsourced computation, then votes to decide computation results’ validity. Validators could be picked through random processes such as through <strong>VRF</strong> (<a href=\"https://en.wikipedia.org/wiki/Verifiable_random_function\" target=\"_blank\" rel=\"noopener\">verifiable random function</a>). </p>\n<p>The following are some literatures describing the technical details of validation:</p>\n<ol>\n<li><strong>Truebit</strong>’s design for validation. Idea 1: offer rewards for checking computations; idea 2: offer rewards for finding bugs; idea 3: offer a bug bounty and provide expectation that bugs will exist. Reference: <a href=\"https://people.cs.uchicago.edu/~teutsch/papers/truebit.pdf\" target=\"_blank\" rel=\"noopener\">A scalable verification solution for blockchains</a></li>\n<li>Weak clients verifying expensive computations: <a href=\"https://www.cs.tau.ac.il/~canetti/CRR11.pdf\" target=\"_blank\" rel=\"noopener\">Referred Delegation of Computation</a></li>\n</ol>\n<p>Another issue regarding computation verification, is that the cost of computation mining is expensive. In order to reduce costs, a miner is prone to deploy limited-memory (or use HDD swaps) environments, or have several virtual machine instances running simultaneously on a same server. This could cause severe QoS issues.<br>A Proof-of-Computing-Resources mechanism is the prerequisites for latency-sensitive computations, such as the Proof-of-Storage used in IPFS. It is used to ensure:</p>\n<ol>\n<li>miner uses general-purpose computing resources (CPU), as ASIC devices can only be used for specific computing.</li>\n<li>colluded mining: miners do not oversell computing resources such as memory, CPU etc. For example, more than one miners share and exploit a same server. Storage service is also susceptible to colluded mining, (similarly two IPFS miners may share a same storage device).</li>\n<li>defense against attacks of computing resources cheating.</li>\n</ol>\n<p><strong>Proof of computing resources</strong> references:</p>\n<ol>\n<li>Proof of computing resources comes from <a href=\"https://hal.inria.fr/hal-01650044\" target=\"_blank\" rel=\"noopener\">asiacrypt 2017</a></li>\n<li>Ron Rivest and Adi Shamir (RS in RSA) in 1990s <a href=\"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.110.5709\" target=\"_blank\" rel=\"noopener\">RSA</a></li>\n</ol>\n<p>Solutions to above-mentioned problems are prerequisites of privacy-preserving computation for sensitive data. System must ensure sensitive  (such as data of movies, musics, economy, market prediction, etc) <strong>information is not compromised by miners</strong>. So far there are 3 proposed solutions:</p>\n<ol>\n<li>homomorphic cryptography</li>\n<li>enclave</li>\n<li>multi-party computing</li>\n</ol>\n<p>One way to protect data privacy, is to handle computation in a fully controlled, trusted environment. By reviewing the algorithm to ensure data is not compromised while processing. This is usually implemented using Intel SGX, which has remote attestation on its execution environment. The context is self-destroyed immediately whenever the environment is accessed from elsewhere. </p>\n<p>To leverage such technology, we can set up a simple session key (DH) with enclave, and let all the data transmitted be encrypted by such key — only the trusted enclave is capable of deciphering it. The trust is derived from Intel hardware to the enclave. The session key serves as a rally point of the trust, ensuring the derived trust can reachw a larger environment setup (we will omit the details here). A proper way to set up the trust environment is critical.</p>\n<p>Enclave is an simpler solution to the problem, but it is based on Intel’s hardware attestation solution. It might sound too heretic to those who firmly holds a decentralized ideology, though. </p>\n<p>Another type of blockchain projects (represented by <strong>Perlin</strong> <a href=\"perlin.net\">perlin.net</a>) leverages homomorphic encryption to ensure privacy-preserving computing. Simply put, the computation result of the homomorphic-encrypted ciphertext can be deciphered only by the encryptor’s private key.  The deciphered result is the same as a computation upon plaintext.<br>Such platform is totally trustless and do not rely on third party attestations.<br>So far its performance is lower than the enclave solution, but performance of homomorphic algorithms can be optimized by GPU. It is expected that the performance will be comparable to enclave in the future.</p>\n<p>Multi-party computing, represented by Project Enigma, is trustless as well. The delivery of Enigma project is two-step: the team will employ enclave technology in the first phase, and them switch to a MPC solution.</p>\n<p>References for Privacy-preserving computing:</p>\n<ol>\n<li>Homomorphic encryption: <a href=\"https://www.theregister.co.uk/2018/03/08/ibm_faster_homomorphic_encryption/\" target=\"_blank\" rel=\"noopener\">Faster Homomorphic Encryption</a></li>\n<li>Enclave: <a href=\"https://software.intel.com/en-us/sgx\" target=\"_blank\" rel=\"noopener\">Intel SGX</a></li>\n<li>MPC: <a href=\"https://dl.acm.org/citation.cfm?id=2954230\" target=\"_blank\" rel=\"noopener\">multi-party computing</a></li>\n</ol>\n<h2 id=\"2018-Observations-on-new-public-blockchain-projects\"><a href=\"#2018-Observations-on-new-public-blockchain-projects\" class=\"headerlink\" title=\"2018: Observations on new public blockchain projects\"></a>2018: Observations on new public blockchain projects</h2><p>New public blockchain projects are gradually shifting away from DPoS consensus. In a high TPS (&gt;10k) blockchain system, a core problem is that <strong>verifier’s computing power is not proportional to that of block producers</strong>. Projects like Bitcoin, Ethereum do not impose expensive costs to verify transactions. In Ethereum, the world state, including contract and contract-internal data are all executed, processed and verified by nodes within the network.   (Reference: <a href=\"https://i.stack.imgur.com/afWDt.jpg\" target=\"_blank\" rel=\"noopener\">https://i.stack.imgur.com/afWDt.jpg</a>)<br>A single full ledger node is capable of handling transaction verifications of about 2K TPS (assuming each transaction has a computing requirement of 1million cpu cycles).</p>\n<p>In a high TPS environment, lesser powerful nodes are not capable of even catch up with the block producing speed of mainnet. Nervos has proposed an asymmetrical solution: If verifying a smart contract takes less time than executing it, then all the network’s transactions are reduced to a decentralized verifying platform. </p>\n<p>You can easily find a lot of similar cases in computer science. For example, it is much harder to solve a sudoku problem than to verify the answer. (Reference: NP problems  <a href=\"https://en.wikipedia.org/wiki/NP_(complexity)\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/NP_(complexity)</a> ). Unfortunately,  such decision problems only have special solutions (such as halting problem). </p>\n<h3 id=\"DAG\"><a href=\"#DAG\" class=\"headerlink\" title=\"DAG\"></a>DAG</h3><p>Directed Acyclical Graphic (DAG) is yet another direction to develop new public blockchains.<br>References:</p>\n<ol>\n<li><a href=\"https://arxiv.org/pdf/1805.03870.pdf\" target=\"_blank\" rel=\"noopener\">Scaling Nakamoto Consensus to Thousands of Transactions per Second</a></li>\n<li>Avalanche: <a href=\"https://ipfs.io/ipfs/QmUy4jh5mGNZvLkjies1RWM4YuvJh5o2FYopNPVYwrRVGV?from=groupmessage&amp;isappinstalled=0\" target=\"_blank\" rel=\"noopener\">https://ipfs.io/ipfs/QmUy4jh5mGNZvLkjies1RWM4YuvJh5o2FYopNPVYwrRVGV?from=groupmessage&amp;isappinstalled=0</a></li>\n<li>Fantom: <a href=\"fantom.foundation/\">fantom.foundation</a></li>\n</ol>\n<p>We would like to elaborate the core problems of the single-chain architecture here:</p>\n<ol>\n<li>PoW is a competitive consensus which recognizes only the longest chain. Failure to be part of the longest chain contributes nothing to the TPS of the network. </li>\n<li>Due to network transmission latency, to shrink block time or to increase block size means to cause a lot of temporary forks. Also, forking splits PoW hashrates, resulting a more vulnerable network, which is more susceptible to double-spending attacks. </li>\n</ol>\n<p>So the core contribution of DAG, is to introduce a new consensus mechanism that makes blocks and transactions on non-longest chains count as part of the network. Supported by mathematical proofs that after several blocks, included transactions reach probabilistic finality, DAG dramatically improves the TPS of the whole network to a sub 10K level.</p>\n<h2 id=\"2018-Observations-on-DApps\"><a href=\"#2018-Observations-on-DApps\" class=\"headerlink\" title=\"2018: Observations on DApps\"></a>2018: Observations on DApps</h2><p>“Traditional” web companies has been actively experimenting blockchain solutions, including: content distribution networks, user generated contents, lending, IoT, games. Even hardware companies joined in to work on things like mesh communication networks, chip-level encryption, etc.</p>\n<p>Our observation is that: it is still too early to embrace a total decentralized token economy. Integration problems still remain unsolved in real-world situation. For example, trust is still an issue when on-chain and off-chain world interact with each other.</p>\n<p>Institutional investors still favor dapps like crypto-wallets, security etc, as they are the second most important traffic entrance other than centralized exchange. Such dapps will soon become a decentralized exchange entrance. However, their token economy design remains a difficulty.</p>\n<p>Lots of projects have a 2B business model; projects deemed as security token; seasonal projects such as World Cup related might explode in 2018.</p>\n<p>Generally speaking, without robust blockchain infrastructures and a reasonable off-chain data authenticity verifying mechanism, related dapps are essentially leveraging company credit and blockchain to improve asset liquidity and transparency.<br>We will continue to keep observing blockchain’s application in all industrial fields. </p>\n","categories":[],"tags":["Blockchain","English"]},{"title":"Binary Translator","url":"lialan.github.io/2016/06/03/binary-translator/","content":"<p>（本文是对知乎问题<a href=\"https://www.zhihu.com/question/29851229/answer/104193305\" target=\"_blank\" rel=\"noopener\">二进制翻译( binary translation )有没有成熟的现实应用？请介绍一下实现方式与性能瓶颈</a>的回复）</p>\n<hr>\n<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>Binary Translation是一个非常大的课题，如果要说得详细了可以写一本书。在这里我们仅仅讨论它的皮毛，也仅仅是大致勾画出二进制翻译器的运行结构。解释有错误的地方还请各位斧正。</p>\n<p>二进制翻译器在业界已然有不少产品使用了，这里只是简单介绍几个。BT之中最著名的可能是Qemu了。还有大家手中各式各样的游戏机模拟器也属于BT，其中，Dolphin模拟器是最成熟的，现在最新的Dolphin已不再是解释执行模拟器了，它已然加入了JIT优化。Intel曾开发过在X86 Android上运行ARM Android app的Houdini，但似乎由于性能问题已不再投入。在中国，计算所胡伟武他们正在做龙芯支持X86的工作（Zhang et al.<br>HERMES: A Fast Cross-ISA Binary<br>Translator with Post-Optimization, CGO’15），也相当不错。</p>\n<p>系统地讲，现有的virtual machine在三个层次上：</p>\n<ul>\n<li>机器层面，比如游戏机模拟器</li>\n<li>系统层面，比如X86 Macintosh运行Power架构的Mac app</li>\n<li>应用层面，比如JVM</li>\n</ul>\n<p>一般来说需要Binary translation（以下简称BT）的地方是guest代码与host机器架构不同的时候（也可以是相同的架构，但那样的翻译大多出于安全或优化而言的，如著名HP Dynamo：Bala et al. Dynamo: A Transparent Dynamic Optimization System PLDI’2000）。</p>\n<h1 id=\"原理\"><a href=\"#原理\" class=\"headerlink\" title=\"原理\"></a>原理</h1><p>下面说说常用的二进制转换的方法和需要注意的问题。BT在执行的过程中需要不做任何假定地考虑：</p>\n<ul>\n<li>冯诺依曼结构计算机的数据和指令不分家，BT如何在执行的过程中区分哪些是数据，哪些是指令呢？</li>\n<li>对会自我修改自我引用的的程序如何处理？</li>\n<li>如何处理间接跳转语句？</li>\n<li>等等等等（一时半会想不起来）…</li>\n</ul>\n<p>二进制转换分两种，一种是静态转换，另一种是动态转换（JIT）。静态转换就是将guest machine的二进制可执行代码翻译成host machine的可执行代码，动态转换就是边执行边翻译。对于静态转换来说，由于没有运行时信息，一个不可能解决的问题是在编译时如何知道间接跳转的目标地址。如果需要完美地转换，一个解决办法是自带一个解释器，当程序跳转到未被翻译指令处时，就启动解释器引擎。当然，这样其实还是需要即时转换的; 另一个办法比较极端，因为我们不知道哪些地址是指令，哪些是数据，我们只能假定所有地址都可以是指令来转换。如果是特定用途的BT，不一定需要这么严格的要求。</p>\n<p>对于动态转换来说，翻译间接跳转成为了可能。由于JIT的特性，对自我修改的程序也能够处理了。以一个动态Binary Translator来说，一种实现方式的伪代码是这样的：</p>\n<ol>\n<li>程序执行流交给BT，BT获得跳转目标地址或入口地址(设名字为tgt)</li>\n<li>BT以tgt所指指令为起始收集接下来的每一条指令，直到找到需要翻译的一段guest指令块</li>\n<li>BT对在第2步获得的block分析，优化，生成host代码<ul>\n<li>对每一条跳转语句，如果目标地址next_tgt已经被转译则更改跳转地址至已转译地址</li>\n<li>如果next_tgt未被转译过，则对该跳转语句生成返回第1步的函数调用，并tgt=next_tgt</li>\n<li>对间接跳转，同样生成一段返回BT的调用。BT查表得到next_tgt之后跳转至第1步</li>\n</ul>\n</li>\n<li>BT将生成的block代码段登记在自己系统里</li>\n<li>跳转到生成的host代码执行</li>\n</ol>\n<p>在第二步收集的指令块的方式决定了指令块的结构，同时也决定了第3步能做哪些优化。通常要求生成的指令块是一个super basic block。SBB与basic block不同的地方在于SBB允许指令流在SBB的中间跳转出去，但因为它和BB一样入口唯一中间没有跳转，仍然保持了块内各指令间的依赖关系，故适合做小范围的优化。<br>当发现程序有自我修改的行为的时候，BT需要将被改写的指令块重新转译。通常做法是在被改写的指令块头插入跳转语句转入新生成的指令块。BT还需要同时修改guest code，以防止自引用的代码出错。</p>\n<h1 id=\"性能与优化\"><a href=\"#性能与优化\" class=\"headerlink\" title=\"性能与优化\"></a>性能与优化</h1><p>不要指望BT的性能会非常好，毕竟它是在跑一个虚拟机。一个naïvely写成的BT要是有host machine的四分之一性能就算很不错了。经过优化后，一般可以达到三分之一。Bansal et al.号称他们用super-optimization可以达到平均三分之二的性能（Binary Translation Using Peephole Superoptimizers OSDI’08）。</p>\n<p>BT的一个重要优化是用 Shadow Stack来预测返回指令的跳转地址，这是BT overhead的一个非常大的一块。注意到返回指令虽然是一条间接指令，通常是要用runtime helper来查表搜索跳转地址的，开销非常大。注意到程序运行时的call stack通常都保存得很完整，每一个函数结束时返回的地址通常都是caller的下一条指令。利用这一个稳定的性质，我们可以用一个缓存栈保存程序调用栈的返回地址。那么下一次转译的返回指令就不需要再用helper查找跳转地址了。</p>\n<p>原理如下：<br>对每个call jump指令，在跳转的同时向ShadowStack这个BT维护的栈中压入一个二元组，元素分别为guest的返回地址和host的返回地址（guestPC+1, hostPC+1）。</p>\n<p>对每个return jump指令，查找SS栈顶的二元组。如果guestPC+1正好是返回地址寄存器RA的值，直接跳转到hostPC+1; 否则弹出SS栈顶搜索下一个。如果超过一定次数或者SS栈为空则跳转到helper查表。</p>\n<p>针对不同的guest architecture还可以进行如下的性能优化（仅举例）：<br>像ARM这样的指令集带有PC relative load，即类似ld r1,10(pc)，可以简化。但要考虑程序的自我修改。</p>\n<p>ARM指令都可以以零成本生成一个算术运算结果的status flag，即NZCV flags，用于比较判断。如果host是像龙芯MIPS这样自身不具有状态寄存器的架构，维护NZCV的开销可以算大头，需要将不需要的状态计算删除。</p>\n<p>如果host machine的寄存器数量低于guest machine（如X86模拟ARM），还需要做合理的register mapping，减少不必要的context switching开销。</p>\n<p>至于其余的代码生成优化，大家可以看上文提到的几篇paper。”Virtual Machines: Versatile Platforms for Systems and Processes”这本书也是很好的资料。</p>\n","categories":[],"tags":["Compiler","中文"]},{"title":"About","url":"lialan.github.io/about/index.html","content":"","categories":[],"tags":[]},{"title":"Tags","url":"lialan.github.io/tag/index.html","content":"","categories":[],"tags":[]},{"title":"search","url":"lialan.github.io/search/index.html","content":"","categories":[],"tags":[]}]